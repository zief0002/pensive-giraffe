[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Instructor/TA",
    "section": "",
    "text": "Instructor: Andrew Zieffler  Email: zief0002@umn.edu  Office: Education Sciences Building 178  Office Hours: Monday 9:00 AM–10:00 AM; and by appointment  Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link.\n\nTA: Carlos Chavez ()  Email: chave143@umn.edu  Office: Education Sciences Building 274  Office Hours: Wednesday 9:00 AM–11:00 AM; and by appointment"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Due Dates",
    "section": "",
    "text": "Assignment\n\n\nDue Date\n\n\nHTML\n\n\n\n\n\n\nAssignment 01: Matrix Algebra for Linear Regression \n\n\nSept. 27\n\n\n\n\n\n\n\nAssignment 02: Simulating from the Regression Model\n\n\nOct. 06\n\n\n\n\n\n\n\nAssignment 03: Regression Diagnostics\n\n\nOct. 13\n\n\n\n\n\n\n\nAssignment 04: Using WLS to Model Data with Outliers\n\n\nOct. 20\n\n\n\n\n\n\n\nAssignment 05: Collinearity and Dimension Reduction\n\n\nNov. 01\n\n\n\n\n\n\n\nAssignment 06: Ridge Regression\n\n\nNov. 15\n\n\n\n\n\n\n\nAssignment 07: Cross-Validation\n\n\nDec. 06\n\n\n\n\n\n\n\nAssignment 08: Path Analysis\n\n\nDec. 15"
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience using matrix algebra to compute various analytic output for regression. In this assignment, you will use the data given below that includes measurements for 10 countries on: infant mortality rate per 1000 live births (infant), the per-capita income (pci) and world region (region) of the country.\n\n\n\n\n\ncountry\ninfant\npci\nregion\n\n\n\n\nAlgeria\n86.3\n400\nAfrica\n\n\nBolivia\n60.4\n200\nAmericas\n\n\nBurundi\n150.0\n68\nAfrica\n\n\nDominican Republic\n48.8\n406\nAmericas\n\n\nKenya\n55.0\n169\nAfrica\n\n\nMalawi\n148.3\n130\nAfrica\n\n\nNicaragua\n46.0\n507\nAmericas\n\n\nParaguay\n38.6\n347\nAmericas\n\n\nRwanda\n132.9\n61\nAfrica\n\n\nTrinidad & Tobago\n26.2\n732\nAmericas\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\n\n\nUnstandardized Regression\nYou will be fitting the model lm(infant ~ 1 + pci + region + pci:region). Within this model, use dummy coding to encode the region predictor and make Americas the reference group.\n\nWrite out the elements of the matrix \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the design matrix.\nDoes \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) have an inverse? Explain.\nCompute (using matrix algebra) and report the vector of coefficients, b for the OLS regression.\nCompute (using matrix algebra) and report the variance–covariance matrix of the coefficients.\nUse the values from b (Question 3) and from the variance–covariance matrix you reported in the previous question to find the 95% CI for the coefficient associated with the main-effect of PCI. (Hint: If you need to refresh yourself on how CIs are computed, see here.)\nCompute (using matrix algebra) and report the hat-matrix, H. Also show how you would use the values in the hat-matrix to find \\(\\hat{y}_1\\) (the predicted value for Algeria).\nCompute (using matrix algebra) and report the vector of residuals, e.\nCompute (using matrix algebra) and report the estimated value for the RMSE.\nGiven the assumptions of the OLS model and the RMSE estimate you computed in the previous question, compute and report the variance–covariance matrix of the residuals.\n\n\n\n\nANOVA Decomposition\nIn this section you will be re-creating the output from the ANOVA decomposition for the model fitted in the previous section.\n\nCompute (using matrix algebra) and report the model, residual, and total sum of squares terms in the ANOVA decomposition table. (2pts)\nCompute (using matrix algebra) and report the model, residual, and total degrees of freedom terms in the ANOVA decomposition table. (2pts)\nUse the values you obtained in Questions 11 and 12 to compute the model and residual mean square terms.\nUse the mean square terms you found in Question 13 to compute the F-value for the model (i.e., to test \\(H_0:\\rho^2=0\\)). Also compute the p-value associated with this F-value. (Hint: If you need to refresh yourself on how F-values or p-values are computed, see here.)\n\n\n\n\nRegression: Effects-Coding\nNow consider fitting the model to the data to examine whether there is an effect of region (no other predictors) on infant mortality. In this model, we will use effects-coding to encode the region variable (see here). This model is often expressed as:\n\\[\n\\mathrm{Infant~Mortality}_i = \\mu + \\alpha_{\\mathrm{Region}} + \\epsilon_i\n\\]\n\nWrite out the design matrix that would be used to fit this model.\nCompute (using matrix algebra) and report the vector of coefficients, b, from the OLS regression.\nCompute (using matrix algebra) and report the variance–covariance matrix for the coefficients.\nExplain why the sampling variances for the coefficients are the same and why the sampling covariance is zero by referring to computations produced in the matrix algebra. (2pts)"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Instructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\n\n\nSimulation 1: Modeling Heteroskedasticity\nIn this simulation, you will explore what happens to the regression estimates when the assumption of homoskedasticity is violated. For this simulation use a sample size of 200.\n\nCreate the fixed X values you will use in each trial of the simulation. To do this, draw \\(n=200\\) X-values from a uniform distribution with a minimum of \\(-2\\) and a maximum of \\(+2\\). Prior to drawing these values, set your starting seed to 678910. Report the syntax you used, and the first six X values.\nCreate a the Y-values for the first trial of the simulation by using the model:\n\n\\[\n\\begin{split}\ny_i &= -3.2 + 1.75(x_i) + \\epsilon_i \\\\[2ex]\n\\epsilon_i &\\overset{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma)\n\\end{split}\n\\]\n\nwhere\n\n\\[\n\\begin{split}\n\\sigma &= e^{\\gamma(x_i)}\\\\[2ex]\ne &\\mathrm{~is~Euhler's~constant~}(\\approx2.718282) \\\\[2ex]\n\\gamma &= 1.5\n\\end{split}\n\\]\n\nHere the variation in the random error is a function of X and random noise. Report the syntax you used, and the first six Y values.\n\n\nCreate and report the scatterplot of the Y-values versus the X-values for this first trial of the simulation.\nDescribe the pattern of heteroscedasticity.\nDoes the pattern of heteroscedasticity you described in Question 4 make sense given how the error term was created. Explain.\n\nCarry out 1000 trials of the simulation. (Reminder: Be sure to use these same X values in each trial of the simulation; they are fixed.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error.\n\nCompute and report the mean value for the residual standard error.\n\n\n\n\nSimulation 2: Homoskedastic Model\nTo evaluate the different estimates from the heteroskedasticity model, we need to compare them to estimates drawn from a homoskedastic model with the same population coefficients. To make the comparisons “fair”, so that we are only evaluating the effects of the heteroskedasticity, we also need to run this simulation using a residual standard error that is equal to the mean from the heteroskedastic simulation (i.e., \\(\\mathtt{sd\\neq1}\\) in the rnorm() function).\n\nCarry out 1000 trials of the simulation for the appropriate homoskedastic model. (Reminder: Be sure to use these same X values in this simulation as in the previous simulation.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error. Report your syntax.\n\n\n\n\nComparing Results from the Two Simulations: Evaluating the Effects of Hetroskedasticity\n\nCreate a density plot of the distribution of intercept estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the intercept.\nBased on your responses to Question 8, does the intercept estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 8, does the intercept estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of slope estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the slope\nBased on your responses to Question 11, does the slope estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 11, does the slope estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of residual standard error estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the residual standard error\nBased on your responses to Question 14, does the residual standard error estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 14, does the residual standard error estimate seem to be less efficient under heteroskedasticity? Explain."
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\n\n\nExploratory Analysis\n\nStart by creating scatterplots to examine the relationship between each of the predictors and the outcome. Are there observations that look problematic in these plots? If so, identify the country(ies).\nFit the regression model (specified in the introduction) to the data. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\n\n\n\n\nOutliers, Leverage, and Influence\n\nCompute the externally studentized residuals for the observations based on the fitted regression. Based on these values, identify any countries that you would consider as regression outliers. Explain why you identified these countries as regression outliers.\nFit a mean-shift model that will allow you to test whether the observation with the largest absolute studentized residual is statistically different from zero. Report the coefficient-level output (B, SE, t, and p) for this model.\nFind (and report) the Bonferroni adjusted p-value for the observation with the largest absolute studentized residual. Based on this p-value, is there statistical evidence to call this observation a regression outlier? Explain.\nCreate and include an index plot of the leverage values. Include a line in this plot that displays the cutpoint for “high” leverage. Based on this plot, identify any countries with large leverage values.\nBased on the evidence you have looked at in Questions #4–7, do you suspect that any of the countries might influence the regression coefficients? Explain.\n\n\n\n\nInfluence Measures\n\nFor each of the influence measures listed below, create and include an index plot of the influence measure. For each plot, also include a line that displays the cutpoint for “high” influence. (2pts)\n\nScaled (standardized) DFBETA values\nCook’s D\nDFFITS\nCOVRATIO\n\nShow how the Cook’s D value for the country with the largest Cook’s D value is calculated using the country’s leverage value and standardized residual.\n\n\n\n\nRemove and Refit\n\nBased on all of the evidence from the different influence measures you examined, identify and report the country(ies) that are influential. Explain how you decided on this set of observations.\nRemove the observations you identified in Question #12 from the data and refit the regression model omitting these observations. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\nCompare and contrast the coefficient-level inferences from the model fitted with the full data and that fitted with the omitted observations.\nCompare and contrast the model-level summaries, namely \\(R^2\\) and the RMSE, from the model fitted with the full data and that fitted with the omitted observations."
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 04",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\n\n\nExploratory Analysis\n\nStart by creating a scatterplot to examine the relationship between socialist party strength and income inequality (outcome).\nAre there observations that look problematic in this plot? If so, identify the country(ies).\nFit a linear model regressing income inequality on socialist party strength. Examine and report a set of regression diagnostics that allow you to identify any observations that are regression outliers.\n\n\n\n\nWeighted Least Squares Estimation\nRather than removing regression outliers from the data, we can instead fit a model that accounts for these observations. For example, fitting a model that allows for higher variance at \\(x\\)-values that have outliers. With higher variances, we would expect more extreme observations because of the increased variance. The WLS model allows for heteroskedasticity and can be used to model data that have extreme observations.\n\nCompute the empirical weights that you will use in the WLS estimation. Report the weight for the United States. (Hint: We do not know the true variances in the population.)\nFit the WLS model. Report the fitted equation.\nBased on the model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nCreate a scatterplot that shows the relationship between socialist party strength and income inequality. Include the country names as labels (or instead of the points). Include both the OLS and WLS regression lines on this plot.\nBased on the plot, comment on how the residuals from the WLS model compare to the residuals from the OLS model.\nBased on your response to Question #8, how will the model-level \\(R^2\\) value from the WLS model compare to the model-level \\(R^2\\) from the OLS model. Explain.\nThe mathematical formulaa for computing the studentized residuals for both the OLS and WLS models is given below. Compute and report the studentized residuals, using this formula, from both the OLS and WLS models for any regression outliers you identified in Question #2. (Hint: Remember that in an OLS regression the weight is 1 for each observation.)\n\n\\[\ne^{\\prime}_i = \\frac{e_i}{s_{e(-i)}\\sqrt{1-h_{ii}}} \\times \\sqrt{w_i}\n\\]\n\nBased on the values of the studentized residuals in the WLS model, are the observations you identified as regression outliers from the OLS model still regression outliers in the WLS model? Why or why not?\nExplain why the is the case by referring to the formula.\nCreate and report residual plots of the studentized residuals versus the fitted values for the OLS and WLS models. Comment on which model better fits the assumptions.\n\n\n\n\nIncluding Covariates\nNow include the energy covariate into the model to examine the effect of socilist strength after controlling for economic development. Since the model has changed, we need to re-compute the weights and re-carry out the WLS analysis.\n\nUse matrix algebra to compute the empirical weights based on the two-predictor model and report the weight for the United States.\nFit the two-predictor WLS model using matrix algebra. Report the fitted equation.\nCompute and report the standard errors of the two-predictor WLS model using matrix algebra.\nUsing your results from Questions #14 and #15, compute and report the t-values and p-values. While you can use the output of the tidy(), summary(), or other functions that automatically compute p-values to check your work, use the pt() function to answer this question. (Show your work or syntax for full credit.)\nBased on the two-predictor WLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nBased on the two-predictor OLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nWhich set of the model results should we trust. Explain by referring to the tenability of the assumptions."
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 05",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 18 points.\n\n\n\nExploratory Analysis\n\nCompute and report the correlation matrix of the 12 predictors.\nBased on the correlations, comment on whether there may be any potential collinearity problems. Explain.\nCompute and report the eigenvalues for the correlation matrix you created in Question 1.\nBased on the eigenvalues, comment on whether there may be any potential collinearity problems. Explain.\n\n\n\n\nInitial Model\nSince the ratings are assigned based on different numbers of attorneys, use a weight equal to the number of respondents to fit a WLS model that regresses the standardized retention percentage on the 12 standardized predictors.\n\nReport the coefficient-level output, including the estimated coefficients (beta weights), standard errors, t-values, and p-values.\nBased on the VIF values, comment on whether there may be any potential collinearity problems. Explain.\nUsing the predictor with the largest VIF value, use the VIF value to indicate how the standard error for this predictor will be impacted by the collinearity.\n\n\n\n\nPrincipal Components Analysis\nIn this section you are going to carry out the principal components analysis by using singular value decomposition on the correlation matrix of the predictors.\n\nCompute the composite score based on the first principal component for the first observation (Judge John J. Bauercamper). Show your work in an equation.\n\nRead the section on scree plots (Section 4) in this web article.\n\nCreate a scree plot showing the eigenvalues for the 12 principal components from the previous analysis.\nUsing the “elbow criterion”, how many principal components are sufficient to describe the data? Explain by referring to your scree plot.\nUsing the “Kaiser criterion”, how many principal components are sufficient to describe the data? Explain.\nUsing the “80% proportion of variance criterion”, how many principal components are sufficient to describe the data? Explain.\n\n\n\n\nRevisit the Regression Analysis\nThe evidence from the previous section suggests that the first two principal components are sufficient to explain the variation in the predictor space.\n\nBy examining the pattern of correlations (size and directions) in the first two principal components, identify the construct defined by the composites of these two components. Explain.\nFit the regression analysis using the first two principal components as predictors of retention percentage. (Don’t forget your weights.) Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?\nAgain, fit the same regression model using the first two principal components as predictors of retention percentage, but this time also include a quadratic effect of the first principal component. Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?\nInterpret the quadratic effect of the first principal component from this model. (It may be helpful to create a plot of the effect to guide your interpretation.)\n\n\n\n\nInfluential Values\n\nBased on Cook’s D, identify the name of any judges (and their Cook’s D value) that are influential observations.\nRemove any influential observations identified in Question 17. Re-fit the same model. Based on comparing the model- and coefficient-level output for this model and the model which included all the observations, comment on how these observations were influencing the \\(R^2\\) value, the estimate of the quadratic effect of PC1, and the effect of PC2."
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "Assignment 06",
    "section": "",
    "text": "The goal of the analysis you are going to undertake in this assignment is to build a model that predicts customers’ credit card balance. To do this you will use the data provided in the file credit.csv. The six predictors included in the dataset have all been previously shown to predict credit card balance."
  },
  {
    "objectID": "assignments/assignment-06.html#model-level-summaries",
    "href": "assignments/assignment-06.html#model-level-summaries",
    "title": "Assignment 06",
    "section": "Model-Level Summaries",
    "text": "Model-Level Summaries\n\nCompute and report the model-level \\(R^2\\) for the ridge regression model. (Hint: Remember that the model-level \\(R^2\\) is the squared correlation between the observed and predicted values of the outcome.) Show your work. How does this compare to the \\(R^2\\) from the OLS model?\nCompute and report the F-value associated with the \\(R^2\\) value you computed in Question #18.\nCompute and report the p-value associated with the test of whether \\(\\rho^2=0\\)."
  },
  {
    "objectID": "assignments/assignment-07.html",
    "href": "assignments/assignment-07.html",
    "title": "Assignment 07",
    "section": "",
    "text": "Part I: Minneapolis Violent Crime\nFor the first part of this assignment, you will use the data provided in the file mpls-violent-crime.csv to build a model that examines the trend in violent crime rate over time.\n\n[CSV]\n[Data Codebook]\n\n\n\nPreparation\nCreate a variable that indicates the number of years since 2000. Use this variable in all analyses for Part I rather than the year variable.\n\n\n\nDescription\n\nCreate a scatterplot showing the violent crime rate as a function of time.\nBased on the plot, describe the trend in violent crime rate over time.\nIf you were going to fit a polynomial model to these data, what degree polynomial would you fit? Explain.\n\n\n\n\nUse p-Value methods for Model Selection\n\nFit a series of polynomial models starting with a linear model, and then models that also include higher order polynomials that allow you to evaluate your response to Question #3. Be sure to fit models up to degree \\(k+1\\), where \\(k\\) is the degree you hypothesized in Question #3. Analyze each of the polynomial terms (including the linear term) by using a series of nested F-tests. Report these results in an ANOVA table. (Note: If you need a refresher on fitting polynomial models and carrying out a nested F-test, see the Polynomial Regression notes from EPsy 8252.)\nBased on these results, which polynomial model would you adopt? Explain.\n\n\n\n\nUsing LOOCV for Model Selection\nIn this section of the assignment, you are going to use LOOCV to evaluate the MSE for the same set of polynomial models you evaluated in Question #4.\n\nWrite and include syntax that will carry out the LOOCV.\nReport the cross-validated MSE for each of the models in your set of polynomial models.\nBased on these results, which degree polynomial model should be adopted? Explain.\n\n\n\n\n\nPart II: Course Evaluations\nFor the second part of this assignment, you will use the data provided in the file evaluations.csv to build a model that predicts variation in course evaluation scores.\n\n[CSV]\n[Data Codebook]\n\n\n\nPreparation\nBegin by fitting a model that predicts average course evaluation score using the following predictors: beauty, number of courses for which the professor has evaluations, whether the professor is a native English speaker, and whether the professor is female.\n\n\n\nDescription\n\nUsing average course evaluation scores (y), compute the total sum of squares (SST). Show your work.\nUsing average course evaluation scores (y) and the predicted values from the model (\\(\\hat{y}\\)), compute the sum of squared errors (SSE). Show your work.\nCompute the model \\(R^2\\) value using the formula: \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\).\n\n\n\n\nUsing k-Fold Cross-Validation to Estimate \\(R^2\\)\nAs we know, the estimate for \\(R^2\\) is biased. We can obtain a better estimate of \\(R^2\\) by using cross-validation. You will use 5-fold cross-validation to estimate the \\(R^2\\) value. The algorithm for this will be:\n\nRandomly divide the evaluations data into 5 folds.\nHold out 1 fold as your validation data and use the remaining 4 folds as your training data.\n\nFit the model to the training data.\nUse the estimated coefficients from those fits to compute \\(\\hat{y}\\) values using the validation data.\nCompute the SST and SSE values for the validation data, and use those to compute \\(R^2\\) based on the formula \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\). (Note that sometimes the \\(R^2\\) may be negative when we compte it in this manner.)\n\nRepeat for each fold.\nCompute the cross-validated \\(R^2\\) by finding the mean of the five \\(R^2\\) from the cross-validations.\n\n\nWrite and include syntax that will carry out the 5-fold cross-validation. In this syntax use set.seed(1000) so that you and the answer key will get the same results. (This website may be useful in using the purrr package to obtain the y- and \\(\\hat{y}\\)-values in order to compute the SST and SSE values: https://drsimonj.svbtle.com/k-fold-cross-validation-with-modelr-and-broom)\nReport the five \\(R^2\\) values from your analysis and the cross-validated \\(R^2\\) value.\nHow does this value compare to the \\(R^2\\) value you computed in Question #11, based on the data.\nExplain why the cross-validated estimate of \\(R^2\\) is a better estimate than the data-based \\(R^2\\).\n\n\n\n\n\nPart III: Credit Balance\nFor the third part of this assignment, you will again use the file the data provided in the file credit.csv to build a model that predicts customers’ credit card balance. Do not forget to standardize all the variables.\n\n[CSV]\n[Data Codebook]\n\n\n\nUse the lm.ridge() function to fit the same sequence of \\(\\lambda\\) values you used in Question #7 from Assignment 6. Running select() on this output, provides \\(\\lambda\\) values based on different criteria. Report the \\(\\lambda\\) value associated with the generalized cross-validation (GCV) metric.\nRe-do Question #7 from Assignment 6, except this time, select the optimal \\(\\lambda\\) value based on using the AICc. How does this compare to the \\(\\lambda\\) value you found using the GCV metric from the previous question? (Show your syntax.)\nCompute the coefficients, standard errors based on the ridge regression model based on the \\(\\lambda\\) value you identified in Question #17. Also compute the t-values, and p-values for each coefficient. Report all of these in a coefficient table."
  },
  {
    "objectID": "assignments/assignment-08.html",
    "href": "assignments/assignment-08.html",
    "title": "Assignment 08",
    "section": "",
    "text": "Background\nTaimalu and Luik (2021)1 used a path model to examine four research questions about “how perceived knowledge about technology integration, and beliefs and attitudes towards using technology, impact the intention to use technology among student teachers” (p. 1). Their model, which is based on the relevant literature is shown here:\n\n\n\n\n\nThey hypothesized that all the path coefficients would be positive. A questionnaire consisting of Likert-scale items related to six constructs was administered to 232 student teachers from the University of Tartu. These constructs, identified in the literature as influencing use of technology, include:\n\nPerceived knowledge about technology integration. These items measure the technological pedagogical knowledge (TPK), technological content knowledge (TCK), and technological pedagogical and content knowledge (TPACK).\nPerceived ease of use (e.g., I find computers easy to use; computer icons are easy to understand for me)\nPerceived usefulness of technology for students (e.g., using technology improves cooperation between the learners);\nPerceived usefulness of technology for teachers (e.g., using technology enhances my effectiveness);\nAttitude toward using technology in education (e.g., technology is valuable in teaching; every teacher must be able to use technology); and\nIntention to use (e.g., I intend to allow learners to use the technology to explore different topics; I intend to guide students to use the Internet to communicate with experts or other learners to enrich their learning experiences).\n\nComposites (using a standardized scale) were created from the items for each construct and the correlation matrix fro these composites (shown below) was used to fit the proposed path model.\n\n\n\n\nIntercorrelations between six measures related to technology integration collected from student teachers. All measures were standardized to have *M* = 0 and *SD* = 1.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n  \n \n\n  \n    1. Intention to use \n    --- \n     \n     \n     \n     \n     \n  \n  \n    2. Perceived knowledge about technology integration \n    0.219 \n    --- \n     \n     \n     \n     \n  \n  \n    3. Perceived ease of use \n    0.33 \n    0.669 \n    --- \n     \n     \n     \n  \n  \n    4. Perceived usefulness for students \n    0.521 \n    0.244 \n    0.364 \n    --- \n     \n     \n  \n  \n    5. Perceived usefulness for teachers \n    0.454 \n    0.309 \n    0.462 \n    0.56 \n    --- \n     \n  \n  \n    6. Attitude toward using \n    0.696 \n    0.181 \n    0.27 \n    0.699 \n    0.53 \n    --- \n  \n\n\n\n\n\n\n\n\nPart 1: Regression from Summary Values\nFor the questions in this section, use the summary values provided to fit a regression to predict variation in intention to use (Model A). Include the predictors of intention to use declared in the path model. Show your syntax for each of these questions.\n\nCompute the standardized regression coefficients for Model A.\nCompute the \\(R^2\\), adjusted-\\(R^2\\) values for Model A.\nCompute the estimated residual standard error for Model A.\nCompute the standard errors for the regression coefficients for Model A.\n\n\n\n\nPart 2: Simulate Data\n\nSimulate the study’s data based on the summary values provided. Assume multivariate normality among the measures. Show the first six rows of the simulated data. Also include your syntax. For reproducibility, set the random starting seed to 57.\n\n\n\n\nPart 3: Path Analysis: All Hypothesized Paths\n\nUse the data you simulated in Part 2 to fit all the relevant regression models in the path analysis. Re-draw the path model and include all the estimated path coefficients, 95% confidence intervals associated with these coefficients, and all error terms/disturbances in the model. (3pts)\n\n\n\n\nPart 4: Path Analysis: Empirically Supported Paths\n\nUse the confidence intervals associated with the path coefficients to evaluate the researchers’ hypotheses that all the effects are positive. Based on these evaluations, re-draw the path model. Omit any paths for hypotheses in the model that are not empirically supported. (2pts)\nCreate a publishable quality table that includes the direct, indirect, and total effects for all of the potential causes of intended technology use in the updated path model. (2pts)\nConsider the following research question: How does student teachers’ perceived ease of use of technology influence their intention to use technology in education?? Use the results from the updated path model to provide an answer to this question. Your response, written in prose (a few sentences), should clearly provide an answer to this question by drawing on the relevant effects in the updated path model. (2pts)\n\n\n\n\n\n\nFootnotes\n\n\nLuik, P., & Taimalu, M. (2021). Predicting the intention to use technology in education among student teachers: A path analysis. Education Sciences, 11(9), 564. https://doi.org/10.3390/educsci11090564↩︎"
  },
  {
    "objectID": "codebooks/contraception.html",
    "href": "codebooks/contraception.html",
    "title": "contraception.csv",
    "section": "",
    "text": "The data in contraception.csv were collected from several sources (e.g., World Bank). The variables are:\n\ncountry: Country name\nregion: Region of the world\ncontraceptive: Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.\neduc_female: Average number of years of formal education (schooling) for females\ngni: Categorical measure of the economy indicating if the country has a low or high gross national income\n\n\n\nPreview\n\n\nCode\n# Import data\ncontraception = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/contraception.csv\")\n\n# View data\ncontraception\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nReferences\n\n\nRoser, M. (2017). Fertility rate. Our world in data.\n\n\nUNICEF. (2016). State of the world’s children 2016. United Nations Population Division’s World Contraceptive Use, household surveys including Demographic and Health Surveys and Multiple Indicator Cluster Surveys.\n\n\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/credit.html",
    "href": "codebooks/credit.html",
    "title": "credit.csv",
    "section": "",
    "text": "The data in credit.csv contains simulated credit card data for 400 individuals. These data are from James et al. (2013). The variables in the data set are:\n\nbalance: Customer’s average credit card balance (in dollars)\nincome: Customer’s reported income (in $10,000 dollars)\nlimit: Credit limit issued to customer\nrating: Customer’s credit rating; higher values indicate a better credit rating\ncards: Number of credit cards the customer has\nage: Customer’s age\neducation: Number of years of education\n\n\n\nPreview\n\n\nCode\n# Import data\ncredit = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/credit.csv\")\n\n# View data\ncredit\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer."
  },
  {
    "objectID": "codebooks/evaluations.html",
    "href": "codebooks/evaluations.html",
    "title": "evaluations.csv",
    "section": "",
    "text": "The data in evaluations.csv come from Hamermesh & Parker (2005) and were made available by Gelman & Hill (2007). This data were collected from student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations. The variables are:\n\nprof_id: Professor ID number\navg_eval: Average course rating\nnum_courses: Number of courses for which the professor has evaluations\nnum_students: Number of students enrolled in the professor’s courses\nperc_evaluating: Average percentage of enrolled students who completed an evaluation\nbeauty: Measure of the professor’s beauty composed of the average score on six standardized beauty ratings\ntenured: Is the professor tenured? (0 = non-tenured; 1 = tenured)\nnative_english: Is the professor a native English speaker? (0 = non-native English speaker; 1 = native English speaker)\nage: Professor’s age (in years)\nfemale: Is the professor female? (0 = not female; 1 = female)\n\n\n\nPreview\n\n\nCode\n# Import data\nevaluations = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/evaluations.csv\")\n\n# View data\nevaluations\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nHamermesh, D. S., & Parker, A. M. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24, 369–376."
  },
  {
    "objectID": "codebooks/iowa-judges.html",
    "href": "codebooks/iowa-judges.html",
    "title": "iowa-judges.csv",
    "section": "",
    "text": "The Iowa State Bar Association has conducted the Judicial Performance Review as a way of giving voters information on the Iowa judges up for retention in an election year. Judges are evaluated on their professional competence and demeanor as determined by the attorneys who frequently appeared before them. The attorneys also indicate whether or not they believe the judge should be retained. The data in iowa-judges.csv were published by the The Iowa State Bar Association (2018). The variables are:\n\nyear: Year of the judicial performance review\njudge: Name of the judge\ndistrict: Judicial district\nrespondents: Number of attorneys who rated the judge\nretention: Percentage of attorneys who indicated the judge should be retained\n\nThe following characteristics were rated on the scale: 5 (Excellent; performance is outstanding), 4 (Good; performance is above average), 3 (Satisfactory; performance is adequate), 2 (Deficient; performance is below average), 1 (Very Poor; performance is well below average and unacceptable).\n\nknowledge: Knowledge and application of the law\nperception: Perception of factual issues\npunctuality: Punctuality for court proceedings\nattention: Attentiveness to evidence and arguments\nmanagement: Management and control of the courtroom\ndemeanor: Temperament and demeanor\nclarity: Clarity and quality of written opinions\npromptness: Promptness of rulings and decisions\n\nThe following characteristics were rated on the scale: 5 (Strongly Agree), 4 (Agree), 3 (Neither), 2 (Disagree), 1 (Strongly Disagree)\n\ncriticism: Avoids undue personal observations or criticisms of litigants, judges and lawyers from bench or in written opinions\ndecision: Decides cases on basis of applicable law and fact, not affected by outside influence.\ncourteous: Is courteous and patient with litigants, lawyers and court personnel.\nequality: Treats people equally regardless of race, gender, age, national origin, religion, sexual orientation, socio-economic status or disability.\n\n\n\nPreview\n\n\nCode\n# Import data\njudges = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/iowa-judges.csv\")\n\n# View data\njudges\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nThe Iowa State Bar Association. (2018). 2018 judicial performance review. The Iowa State Bar Association. https://cdn.ymaws.com/www.iowabar.org/resource/resmgr/judicial_performance_review/2018/2018_Judicial_Performance_Re.pdf"
  },
  {
    "objectID": "codebooks/mpls-violent-crime.html",
    "href": "codebooks/mpls-violent-crime.html",
    "title": "mpls-violent-crimes.csv",
    "section": "",
    "text": "The mpls-violent-crime.csv file contains data collected from the Minneapolis Police Department and reported by the Star Tribune on The two attributes in this file are:\n\nyear: Year\ncrime_rate: Violent crime rate per 100,000 people\n\n\n\nPreview\n\n\nCode\n# Import data\ncrime = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/mpls-violent-crime.csv\")\n\n# View data\ncrime"
  },
  {
    "objectID": "codebooks/slid.html",
    "href": "codebooks/slid.html",
    "title": "slid.csv",
    "section": "",
    "text": "The data in slid.csv includes a subset of data collected during the 1994 wave of Statistics Canada’s Survey of Labour and Income Dynamics (SLID). These data constitute employed citizens living in Ontario, Canada between the ages of 16 and 65. These data are taken from the public-use dataset made available by Statistics Canada, and prepared by the Institute for Social Research, York University. They were made available in Fox et al. (2022). The variables in the data set are:\n\nwages: Composite hourly wage rate based on all the participant’s jobs\nage: Age of the participant (in years)\neducation: Number of years of schooling\nmale: A dummy-coded predictor for sex (0=Non-male; 1=Male)\n\n\n\nPreview\n\n\nCode\n# Import data\nslid = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/slid.csv\")\n\n# View data\nslid\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFox, J., Weisberg, S., & Price, B. (2022). CarData: Companion to applied regression data sets. R package version 3.0-5. https://CRAN.R-project.org/package=carData"
  },
  {
    "objectID": "codebooks/stack-1979.html",
    "href": "codebooks/stack-1979.html",
    "title": "stack-1979.csv",
    "section": "",
    "text": "Stack (1979) studied predictors of income inequality in a paper published in the American Sociological Review. He posited that more political participation and a strong Socialist party in a country would be associated with less income inequality. To control for variation in economic development, he included a measure of energy consumption, which had been identified as a reasonable economic proxy in previous studies. Stack’s data, stored in stack-1979.csv, include four attributes measured on 18 countries. The attributes are:\n\ncountry: Country name\ninequality: Ratio of the share of income received by the most wealthy population quintile (richest 20%) to the share received by the poorest 40% of the population; Higher values indicate more income inequality\nturnout: Proportion of the adult population voting in the most recent national election prior to 1972\nenergy: Energy consumption per capita (expressed in million metric tons of coal equivalents; higher values indicate more economic development\nsocialist: Annual average proportion of seats held by socialist parties in the national legislature, over the first twenty postwar years\n\n\n\nData Preview\n\n\nCode\n# Import data\nstack = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/stack-1979.csv\")\n\n# View data\nstack\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nStack, S. (1979). The effects of political participation and socialist party strength on the degree of income inequality. American Sociological Review, 44(1), 168–171."
  },
  {
    "objectID": "codebooks/states-2019.html",
    "href": "codebooks/states-2019.html",
    "title": "states-2019.csv",
    "section": "",
    "text": "The data in states-2019.csv include statistics collected from Wikipedia, the 2019 American Community Survey, and the National Centers for Environmental Information. The attributes in the data are:\n\nstate: State/territory name\nlife_expectancy: Life expectancy (in years)\npopulation: Population estimate (in millions)\nincome: Per capita income (in thousands of dollars)\nilliteracy: Illiteracy rate (in percent)\nmurder: Murder and non-negligent manslaughter rate (per 100,000 population)\nhs_grad: Percentage of high school graduates\nfrost: Mean number of days with minimum temperature below freezing\narea: Land area (in square miles)\n\n\n\nPreview\n\n\nCode\n# Import data\nusa = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/states-2019.csv\")\n\n# View data\nusa\n\n\n\n\n  \n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Name\n\n\nData\n\n\nCodebook\n\n\n\n\n\n\ncontraception.csv\n\n\n  \n\n\n  \n\n\n\n\ncredit.csv\n\n\n  \n\n\n  \n\n\n\n\nequal-education-opportunity.csv\n\n\n  \n\n\n\n\n\n\n\n\n\nevaluations.csv\n\n\n  \n\n\n  \n\n\n\n\niowa-judges.csv\n\n\n  \n\n\n  \n\n\n\n\nmpls-violent-crime.csv\n\n\n  \n\n\n  \n\n\n\n\nslid.csv\n\n\n  \n\n\n  \n\n\n\n\nstack-1979.csv\n\n\n  \n\n\n  \n\n\n\n\nstates-2019.csv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPsy 8264",
    "section": "",
    "text": "Welcome to EPsy 8264: Advanced Multiple Regression Analysis. This is an advanced seminar for doctoral students in education covering a diverse set of regression methodologies. We will begin with a brief review of the General Linear Model and establishment of a mathematical foundation for the estimation of regression coefficients and standard errors in these models through the use of matrix algebra. The course will also cover more advanced modeling techniques, such as regression diagnostics, WLS and sandwich estimation, PCA, shrinkage methods, model selection and local models.\n\n\n\n\n\n\nTuesday/Thursday (9:45–11:00): Appleby Hall 204\n\n\n\n\n\n\nThe course syllabus is available here.\nMessage from Snoop Dogg about the syllabus\n\n\n\n\n\nThe course textbook is available via the University of Minnesota library.\n\nFox, J. (2013). A mathematical primer for social statistics. Sage."
  },
  {
    "objectID": "notes/03-ols-regression.html",
    "href": "notes/03-ols-regression.html",
    "title": "📝 OLS Regression Using Matrices and Its Properties",
    "section": "",
    "text": "Summation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this.\n\nThe handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest."
  },
  {
    "objectID": "notes/04-simulating-from-the-regression-model.html",
    "href": "notes/04-simulating-from-the-regression-model.html",
    "title": "📝 Simulating from the Regression Model",
    "section": "",
    "text": "Generating Data\n\nGenerating Random Data from a Regression Model is a script file that provides syntax for generating data from a given population regression model.\n\n\n\n\nSimulation 1: Simulating from a Regression Model\n\nSimulating from a Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates from a regression model.\nVizualization of Simulating from a Regression Model is a handout visualizing the simulation process for generating data from a given population regression model.\n\n\n\n\nSimulation 2: Simulating from a Null Regression Model\n\nSimulating from a Null Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates assuming certain parameters in the regression model are zero.\nVizualization of Simulating from a Null Regression Model is a handout visualizing the simulation process for generating data to produce distributions of estimates assuming certain parameters in the regression model are zero.\n\n\n\n\nResources\nHere are several resources to help your understanding of simulation. The chapters from Monte Carlo Simulation and Resampling Methods for Social Science should be accessible via the links after logging in with your x500 and password.\n\nProbability: Common probability distributions and how to compute with them.\nRandom Number Generation: Learn how to draw random numbers from different distributions. Also information about repeating processes in R, including writing your own functions, using for loops, and using if-else functions."
  },
  {
    "objectID": "notes/05-regression-diagnostics.html",
    "href": "notes/05-regression-diagnostics.html",
    "title": "📝 Regression Diagnostics",
    "section": "",
    "text": "Slides is a set of slides we will cover in class.\nScript File is a script file that provides syntax for generating data from a given population regression model."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "",
    "text": "In this set of notes, we will use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID) to explain variation in the hourly wage rate of employed citizens in Ontario.\nA script file for the analyses in these notes is also available:"
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Violating Homoskedasticity",
    "text": "Violating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\n\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\n\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes)."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nIs there a power transformation that would better “fix” the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis transformation is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\).\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\nslid = slid %>%\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n\nCode\n# Examine residual plots\nresidual_plots(lm_bc)\n\n\n\n\n\nResidual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficient-level output:\n\ntidy(lm_bc, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y, controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?"
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Profile Plot for Different Transformations",
    "text": "Profile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\nPlot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n# Zomm in on confidence limits\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\nPlot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Assume Error Variances are Known",
    "text": "Assume Error Variances are Known\nLet’s assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance–covariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\ndata.frame(\n  class_act = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2),\n  teacher_act = c(21, 20 , 19, 18, 17, 16),\n  class_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\n) %>%\n  kable(\n    col.names = c(\"Class Average ACT\", \"Teacher ACT\", \"Class SD\"),\n    align = \"c\"\n  ) %>%\n  kable_classic()\n\n\n\n \n  \n    Class Average ACT \n    Teacher ACT \n    Class SD \n  \n \n\n  \n    17.3 \n    21 \n    5.99 \n  \n  \n    17.1 \n    20 \n    3.94 \n  \n  \n    16.4 \n    19 \n    1.90 \n  \n  \n    16.4 \n    18 \n    0.40 \n  \n  \n    16.1 \n    17 \n    5.65 \n  \n  \n    16.2 \n    16 \n    2.59 \n  \n\n\n\n\n\nSuppose we want to use the teacher’s ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1)\nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n[1] 0.98356794 0.05294073\n\n\nWe could also have used built-in R functions to obtain these values:\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n[1] 1.17680463 0.06527187\n\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\n \n\n\nOLS\nWLS\n\n  \n    Coefficient \n    B \n    SE \n    B \n    SE \n  \n \n\n  \n    Intercept \n    12.0905 \n    0.9836 \n    13.4155 \n    1.1768 \n  \n  \n    Effect of Teacher ACT Score \n    0.2429 \n    0.0529 \n    0.1658 \n    0.0653 \n  \n\n\n\n\n\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n\n\n  \n\n\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "What if Error Variances are Unknown?",
    "text": "What if Error Variances are Unknown?\nThe previous example assumed that the variance–covariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\n\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\n\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) %>%\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\nBefore examining any output from this model, let’s examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n\nCode\n# Examine residual plots\nresidual_plots(lm_step_5)\n\n\n\n\n\nResidual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)"
  },
  {
    "objectID": "readings/01-welcome-to-8264.html",
    "href": "readings/01-welcome-to-8264.html",
    "title": "📖 Welcome to EPsy 8264",
    "section": "",
    "text": "In this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning.\nThe TAs and I will do everything we can to help with this, but, as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "readings/01-welcome-to-8264.html#prerequisites",
    "href": "readings/01-welcome-to-8264.html#prerequisites",
    "title": "📖 Welcome to EPsy 8264",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe pre-requisites for this course are EPsy 8251 and EPsy 8252. Prerequisite knowledge include topics from a basic statistics course:\n\nFoundational topics in data analysis;\n\nDesign (e.g., random assignment and random sampling)\nDescriptive statistics and plots\nOne- and two-sample tests\n\n\nAnd, topics from EPsy 8251: Methods in Data Analysis for Educational Research I:\n\nStatistical Computation\n\nUsing R\nData wrangling/manipulation\nPlotting\n\nCorrelation;\nSimple regression analysis;\n\nModel-level and coefficient-level interpretation\nOrdinary least squares estimation\nStandardized regression\nPartitioning sums of squares\nModel-level and coefficient-level inference\nAssumption checking/residual analysis\n\nMultiple linear regression\n\nModel-level and coefficient-level interpretation and inference\nAssumption checking/residual analysis\nWorking with categorical predictors (including adjusting p-values for multiple tests)\nInteraction effects\n\n\nAnd topics from EPsy 8252: Methods in Data Analysis for Educational Research II:\n\nDealing with nonlinearity;\n\nQuadratic effects\nLog-transformations\n\nProbability distributions;\n\nProbability density\n\nMaximum likelihood estimation;\nModel selection;\n\nInformation criteria\n\nLinear mixed-effects models (cross-sectional/longitudinal)\n\nBasic ideas of mixed-effects models\nFitting models with random-intercepts and random-slopes\nAssumptions\nLikelihood ratio tests\n\nGeneralized linear models\n\nLogistic models"
  },
  {
    "objectID": "readings/01-welcome-to-8264.html#resources",
    "href": "readings/01-welcome-to-8264.html#resources",
    "title": "📖 Welcome to EPsy 8264",
    "section": "Resources",
    "text": "Resources\nFor the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. If you need a refresher on any of these topics, see:\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists [EPsy 8251 material]\nEPsy 8252 website"
  },
  {
    "objectID": "readings/02-introduction-to-matrix-algebra.html",
    "href": "readings/02-introduction-to-matrix-algebra.html",
    "title": "📖 Introduction to Matrix Algebra",
    "section": "",
    "text": "Introduction\nData Structures\nVectors\nVector Operations\n\nIn class, we will be working through some problems to cement these ideas. We will also examine and work through some problems related to matrix operations, so you could also read through:\n\nMatrices\nMatrix Addition and Subtraction\nMatrix Multiplication\nMatrix Transposition"
  },
  {
    "objectID": "readings/03-ols.html",
    "href": "readings/03-ols.html",
    "title": "📖 OLS Regression using Matrices and its Properties",
    "section": "",
    "text": "Systems of Equations\nStatistical Application: Estimating Regression Coefficients\n\nIn class, we will be working through the ideas in the following chapters:\n\nImportant Matrices in Regression\nSums of Squares in Regression\nStandard Errors and Variance Estimates\nAssumptions of the Regression Model"
  },
  {
    "objectID": "readings/04-simulating-from-the-regression-model.html",
    "href": "readings/04-simulating-from-the-regression-model.html",
    "title": "📖 Simulating from the Regression Model",
    "section": "",
    "text": "Introduction: This chapter gives a short introduction to the use of simulation in the social sciences."
  },
  {
    "objectID": "readings/05-regression-diagnostics.html",
    "href": "readings/05-regression-diagnostics.html",
    "title": "📖 Regression Diagnostics",
    "section": "",
    "text": "Fox, J. (1991). Outlying and influential data. In Regression diagnostics (pp. 21–40). Sage. doi: https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781412985604.n4\n\n\n\nAdditional Resources\n\nKim, B. (2015). Understanding diagnostic plots for linear regression analysis. University of Virginia Library.\nCook, R. D. (1998). Regression graphics: Ideas for studying regressions through graphics. Wiley."
  },
  {
    "objectID": "readings/06-variance-stabilizing-transformations.html",
    "href": "readings/06-variance-stabilizing-transformations.html",
    "title": "📖 Variance Stabilizing Transformations",
    "section": "",
    "text": "Osborne, J. W. (2009). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6).\n\n\n\nAdditional Resources\n\nKaufman, R. L. (2013). Heteroskedasticity in regression: Detection and correction. Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4"
  },
  {
    "objectID": "readings/07-diagnosing-collinearity.html",
    "href": "readings/07-diagnosing-collinearity.html",
    "title": "📖 Diagnosing Collinearity",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Eigenvalues and Eigenvectors. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nCook, D. (2019). How to use a tour to check if your model suffers from multicollinearity. Personal blog."
  },
  {
    "objectID": "readings/07-wls-and-sandwich-estimation.html",
    "href": "readings/07-wls-and-sandwich-estimation.html",
    "title": "📖 Weighted Least Squares (WLS) and Sandwich Estimation",
    "section": "",
    "text": "Read Kaufman, R. L. (2013). Heteroskedasticity-consistent (robust) standard errors. In Heteroskedasticity in regression: Detection and correction (pp. 43–50). Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\n\n\nAdditional Resources\n\nShin, H.-C. (1998). Weighted least squares estimation with sampling weights. Journal of Econometrics, 8(2), 251–271.\nSolon, G., Haider, S. J., & Woolridge, J. (2013). What are we weighting for? (Working Paper No. 18859; NBER Working Paper Series). National Bureau of Economic Research."
  },
  {
    "objectID": "readings/08-pca-via-spectral-decomposition.html",
    "href": "readings/08-pca-via-spectral-decomposition.html",
    "title": "📖 Principal Components Analysis via Spectral Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Basis vectors and matrices. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and eigenvectors. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Spectral decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nWilke, C. O. (2020). PCA tidyverse style. Personal blog."
  },
  {
    "objectID": "readings/09-pca-via-svd.html",
    "href": "readings/09-pca-via-svd.html",
    "title": "📖 Principal Components Analysis via Singular Value Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Singular value decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nGundersen, G. (2018). Singular value decomposition as simply as possible.\nWang, Z. (2019). PCA and SVD explained with numpy.\nWikipdeia. (2020). Singular value decomposition."
  },
  {
    "objectID": "readings/10-biased-estimation-ridge-regression.html",
    "href": "readings/10-biased-estimation-ridge-regression.html",
    "title": "📖 Biased Estimation: Ridge Regression",
    "section": "",
    "text": "Fortmann-Roe, S. (2012). Understanding the bias-variance tradeoff.\n\n\n\nAdditional Resources\n\nCross-Validated. (2014). Why is ridge regression called “ridge”, why is it needed, and what happens when \\(\\lambda\\) goes to infinity?.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.\nStatistical Learning MOOC taught by Hastie and Tibshirani"
  },
  {
    "objectID": "readings/11-model-selection.html",
    "href": "readings/11-model-selection.html",
    "title": "📖 Model Selection",
    "section": "",
    "text": "Heinze, G., Wallisch, C., & Dunkler, D. (2018). Variable selection — A review and recommendations for the practicing statistician. Biometrical Journal. Biometrische Zeitschrift, 60(3), 431–449. https://doi.org/10.1002/bimj.201700067\n\n\n\nAdditional Resources\n\nolsrr Vignette\nWilliams, B., Hansen, G., Baraban, A., & Santoni, A. (2015). A practical approach to variable selection—A comparison of various techniques. Casualty Actuarial Society E-Forum, Summer, 1–20."
  },
  {
    "objectID": "readings/12-cross-validation.html",
    "href": "readings/12-cross-validation.html",
    "title": "📖 Cross-Validation",
    "section": "",
    "text": "Feick, L. (2019). Evaluating model performance by building cross-validation from scratch. STATWORX Blog.\n\n\n\nAdditional Resources\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). Cross-validation. In An introduction to statistical learning: with applications in R (pp. 176–186). New York: Springer."
  },
  {
    "objectID": "readings/13-regression-from-summary-measures.html",
    "href": "readings/13-regression-from-summary-measures.html",
    "title": "📖 Regression from Summary Measures",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Statistical application: SSCP, variance–covariance, and correlation matrices.. Matrix Algebra for Educational Scientists.\n\nIn particular pay attention to the relationships between these three matrices."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Calendar",
    "section": "",
    "text": "Date\n\n\nReading\n\n\nTopic\n\n\nNotes\n\n\n\n\n\n\n \n\n\nSept. 06\n\n\n\n\n\nWelcome to EPsy 8264\n\n\n \n\n\n\n\n\nUnit 01: Mathematical and Computational Foundations\n\n\n\n\n \n\n\nSept. 08\n\n\n\n\n\nIntroduction to Matrix Algebra\n\n\n\n\n\n\n\n \n\n\nSept. 13\n\n\n\n\n \n\n\nSept. 15\n\n\n\n\n\nOLS Regression using Matrices and its Properties\n\n\n\n\n\n\n\n \n\n\nSept. 20\n\n\n\n\n\nUnit 02: Simulation\n\n\n\n\n \n\n\nSept. 22\n\n\n\n\n\nSimulating from the Regression Model\n\n\n\n\n\n\n\n \n\n\nSept. 27\n\n\n\n\n \n\n\nSept. 29\n\n\n\n\n\nUnit 03: Regression Diagnostics\n\n\n\n\n \n\n\nOct. 04\n\n\n\n\n\nRegression Diagnostics\n\n\n\n\n\n\n\n \n\n\nOct. 06\n\n\n\n\n\nUnit 04: Tools for Dealing with Heteroskedasticity\n\n\n\n\n \n\n\nOct. 11\n\n\n\n\n\nVariance Stabilizing Transformations\n\n\n\n\n\n\n\n \n\n\nOct. 13\n\n\n\n\n\nWeighted Least Squares (WLS) and Sandwich Estimation"
  },
  {
    "objectID": "worksheets/02-introduction-to-matrix-algebra.html",
    "href": "worksheets/02-introduction-to-matrix-algebra.html",
    "title": "💪 Introduction to Matrix Algebra",
    "section": "",
    "text": "Problems\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & -2\\\\5 & 1\\end{bmatrix} \\quad \\mathbf{B} = \\begin{bmatrix}3 & -1\\\\-1 & 2\\end{bmatrix}\\quad \\mathbf{C} = \\begin{bmatrix}1 & 2 & 3\\\\0 & 1 & 2\\end{bmatrix}\n\\]\nMake sure everyone in your group can solve each of these problem by hand and using R.\n\nWhat are the dimensions of A? C?\nIs C a square matrix? Explain.\nFind the trace of A.\nFind the determinant of A.\nAdd A and B\nFind the transpose of C.\nBy referring to the dimensions, can you compute AC? How about CA?\nCompute AC.\nCompute BI\nCreate a \\(3\\times3\\) diagonal matrix whose trace is 10.\nHow do you know that B has an inverse? Explain.\nCompute \\(\\mathbf{B}^{-1}\\)\nCreate a \\(3\\times3\\) matrix that has rank 2. Verify this using R.\nCreate a \\(3\\times3\\) matrix that is symmetric and is not I.\nSolve the system of linear equations using algebra (e.g., substitution, elimination) and then solve them using matrix methods (with R). To do this you will need to read the Systems of Equations chapter in Matrix Algebra for Educational Scientists.\n\n\\[\n\\begin{split}\nx + y + z &= 2 \\\\\n6x - 4y + 5z &= 31 \\\\\n5x + 2y + 2z &= 13\n\\end{split}\n\\]"
  },
  {
    "objectID": "worksheets/13-model-selection.html",
    "href": "worksheets/13-model-selection.html",
    "title": "💪 Model Selection (In-Class Activity)",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\nYour goal is to use the data (and everything you’ve learned so far in your coursework) to create a model to predict variation in life expectancy. Keep track of the process you use to create this model, including:\n\nWhich predictors should be included in the model?\n\nWhat is the criteria/evidence you are using to make these decisions?\n\nWhen in the process do you identify problematic observations?\n\nDo you remove those problematic observations or not?\nWhat criteria/evidence are you using to make these decisions?\n\nWhen in the process do you examine the model for collinearity?\n\nWhat is the criteria/evidence you are using to make this decision?\nWhat (if anything) will you do to fix this?\n\nWhen in the process do you examine the tenability of assumptions?\n\nAlso pay attention to when in the process you are making decisions based on sample evidence (graphs/statistics) versus when those decisions are being made using statistical inference (hypothesis tests, confidence intervals). Your group will be asked to report back to the class on the process, criteria, and evidence you used."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html",
    "href": "handouts/handout-regression-example-in-practice.html",
    "title": "A Regression Example",
    "section": "",
    "text": "In this document we will use the data in contraception.csv to examine whether female education level explains variation in contraceptive useage after controlling for GNI."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#examine-the-data",
    "href": "handouts/handout-regression-example-in-practice.html#examine-the-data",
    "title": "A Regression Example",
    "section": "Examine the Data",
    "text": "Examine the Data\nWe need to correctly specify the model. Since we have no theory to guide us, this is done empirically by looking at the data.\n\n# Create scatterplot\np = ggplot(data = contraception, aes(x = educ_female, y = contraceptive)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) \n\n# Add marginal density plots\nggMarginal(p, type = \"density\")\n\n# Condition the relationship on GNI\nggplot(data = contraception, aes(x = educ_female, y = contraceptive, color = gni)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) +\n   facet_wrap(~gni)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould we include main-effects only? Or an interaction?\nIs there non-linearity to account for (e.g., transformations)? Or does it look linear?"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "href": "handouts/handout-regression-example-in-practice.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "title": "A Regression Example",
    "section": "Use Matrix Algebra to Compute Coefficient Estimates",
    "text": "Use Matrix Algebra to Compute Coefficient Estimates\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\n\n# Store values\nn = nrow(contraception) #Sample size\nk = 2 #Number of predictors\n\n\n# Create outcome vector\ny = contraception$contraceptive\n\n# Create dummy variable for GNI\ncontraception = contraception %>%\n   mutate(\n      high_gni = if_else(gni == \"High\", 1, 0)\n      )\n\n# Create design matrix\nX = matrix(\n   data = c(rep(1, n), contraception$educ_female, contraception$high_gni),\n   ncol = 3\n)\n\n# Compute b vector\nb = solve(t(X) %*% X) %*% t(X) %*% y\nb\n\n          [,1]\n[1,] 27.021387\n[2,]  4.088735\n[3,]  1.608766\n\n\nThus the fitted regression equation is:\n\\[\n\\widehat{\\mathrm{Contraceptive~Use}}_i = 27.02 + 4.09(\\mathrm{Female~Education~Level}_i) + 1.60(\\mathrm{High~GNI}_i)\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#compute-residual-standard-error",
    "href": "handouts/handout-regression-example-in-practice.html#compute-residual-standard-error",
    "title": "A Regression Example",
    "section": "Compute Residual Standard Error",
    "text": "Compute Residual Standard Error\n\n# Compute e vector\ne = y - X %*% b\n\n# Compute s_e\ns_e = sqrt((t(e) %*% e) / (n - k - 1))\ns_e\n\n         [,1]\n[1,] 14.39792\n\n\nThus the residual standard error (a.k.a., the root mean square error; RMSE) is:\n\\[\ns_e = 14.40\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#compute-variancecovariance-matrix-for-the-coefficients",
    "href": "handouts/handout-regression-example-in-practice.html#compute-variancecovariance-matrix-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Compute Variance–Covariance Matrix for the Coefficients",
    "text": "Compute Variance–Covariance Matrix for the Coefficients\n\\[\n\\mathrm{Var}(\\mathbf{b}) = s^2_e(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\]\nwhere \\(s^2_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\\)\n\n# Compute varaince-covariance matrix of b\nV = as.numeric(s_e^2) * solve(t(X) %*% X)\nV\n\n          [,1]       [,2]      [,3]\n[1,] 12.414688 -1.8783934  4.782603\n[2,] -1.878393  0.4267136 -2.028306\n[3,]  4.782603 -2.0283060 18.197825\n\n# Compute SEs for b\nsqrt(diag(V))\n\n[1] 3.5234483 0.6532332 4.2658909\n\n\nThus\n\\[\n\\mathrm{SE}(b_0) = 3.52 \\qquad \\mathrm{SE}(b_1) = 0.65 \\qquad \\mathrm{SE}(b_2) = 4.27\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#coefficient-level-inference",
    "href": "handouts/handout-regression-example-in-practice.html#coefficient-level-inference",
    "title": "A Regression Example",
    "section": "Coefficient-Level Inference",
    "text": "Coefficient-Level Inference\nHere we will focus on the effects of female education level since it is our focal predictor. (GNI is a control.) Note this is the second effect in the b vector and in the V matrix. We will test the hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\n\n# Compute t-value\nt_0 = (b[2] - 0) / sqrt(V[2, 2])\nt_0\n\n[1] 6.259228\n\n# Evaluate t-value\ndf = n - k - 1\np = 2* (1 - pt(abs(t_0), df = df))\np\n\n[1] 0.00000001143799\n\n\nHere,\n\\[\nt(94) = 6.26,~p=0.0000000114\n\\]\nThe evidence suggests that the data are not very compatible with the hypothesis that there is no effect of female education level on contraceptive useage, after controlling for differences in GNI."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "href": "handouts/handout-regression-example-in-practice.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Statistical Inference: Confidence Intervals for the Coefficients",
    "text": "Statistical Inference: Confidence Intervals for the Coefficients\nFrom the hypothesis test, we believe there is an effect of female education level on contraceptive useage, after controlling for differences in GNI. What is that effect? To answer this we will compute a 95% CI for the effect of female education.\n\n# Compute critical value\nt_star = qt(.025, df = df)\n\n# Compute CI\nb[2] - abs(t_star) * sqrt(V[2, 2])\n\n[1] 2.791725\n\nb[2] + abs(t_star) * sqrt(V[2, 2])\n\n[1] 5.385745\n\n\nThe 95% CI indicates that the population effect of female education level on contraceptive useage, after controlling for differences in GNI is between 2.79 and 5.39."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#accessing-regression-matrices-from-lm",
    "href": "handouts/handout-regression-example-in-practice.html#accessing-regression-matrices-from-lm",
    "title": "A Regression Example",
    "section": "Accessing Regression Matrices from lm()",
    "text": "Accessing Regression Matrices from lm()\nThere are several built-in R functions that allow you to access different regression matrices once you have fitted a model with lm().\n\n# Access design matrix\nmodel.matrix(lm.1)\n\n   (Intercept) educ_female gniLow\n1            1         5.9      0\n2            1         8.9      0\n3            1        10.5      0\n4            1         4.6      1\n:            :          :       :\n97           1         6.7      1\nattr(,\"assign\")\n[1] 0 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gni\n[1] \"contr.treatment\"\nThe design matrix is given and information about this design matrix is also encoded. There is an attribute “assign”, an integer vector with an entry for each column in the matrix giving the term in the formula which gave rise to the column. Value 0 corresponds to the intercept (if any), and positive values to terms in the order given by the term.labels attribute of the terms structure corresponding to object. There is also an attribute called “contrasts” that identifies any factors (categorical variables) in the model and indicates how the contrast testing (comparison of the factor levels) will be carried out. Here “contr.treatment” is used. This compares each level of the factor to the baseline (which is how dummy coding works).\n\n# Access coefficient estimates\ncoef(lm.1)\n\n(Intercept) educ_female      gniLow \n  28.630153    4.088735   -1.608766 \n\n# Access variance-covariance matrix for b\nvcov(lm.1)\n\n            (Intercept) educ_female     gniLow\n(Intercept)   40.177719  -3.9066994 -22.980428\neduc_female   -3.906699   0.4267136   2.028306\ngniLow       -22.980428   2.0283060  18.197825\n\n# Access fitted values\nfitted(lm.1)\n\n       1        2        3        4        5        6        7        8 \n52.75369 65.01990 71.56187 45.82957 71.56187 66.24652 35.19886 61.36676 \n       9       10       11       12       13       14       15       16 \n58.06905 64.20215 71.97075 34.78998 36.01660 40.10534 47.87394 78.92160 \n      17       18       19       20       21       22       23       24 \n29.47463 67.88201 57.25130 35.60773 62.97553 71.56187 78.10385 60.11341 \n      25       26       27       28       29       30       31       32 \n58.88679 48.69168 51.96267 32.74562 73.19737 51.14493 69.10863 30.29238 \n      33       34       35       36       37       38       39       40 \n40.10534 48.69168 74.42399 40.10534 55.23366 46.62059 68.29089 68.69976 \n      41       42       43       44       45       46       47       48 \n74.42399 67.06427 70.33525 49.10056 74.01512 42.55858 59.70454 54.82479 \n      49       50       51       52       53       54       55       56 \n36.42548 46.64732 40.92309 66.24652 50.70932 32.74562 67.47314 61.34004 \n      57       58       59       60       61       62       63       64 \n61.74891 66.27325 61.77564 40.10534 30.29238 54.38919 36.83435 46.64732 \n      65       66       67       68       69       70       71       72 \n30.29238 44.19408 40.51421 67.88201 59.29567 63.00226 61.34004 71.15300 \n      73       74       75       76       77       78       79       80 \n39.69647 43.37633 40.92309 66.24652 35.19886 76.05948 76.87723 68.69976 \n      81       82       83       84       85       86       87       88 \n67.47314 58.47792 57.27803 67.90874 45.42070 57.25130 40.51421 49.50943 \n      89       90       91       92       93       94       95       96 \n44.60295 72.81522 80.96597 64.20215 64.20215 48.28281 31.92787 50.73605 \n      97 \n54.41591 \n\n# Access raw residuals\nresid(lm.1)\n\n          1           2           3           4           5           6 \n  4.2463087   0.9801026 -16.5618739  16.1704304  -4.5618739 -15.2465180 \n          7           8           9          10          11          12 \n-19.1988577   5.6332361 -12.0690473 -11.2021503  -2.9707474  -2.7899842 \n         13          14          15          16          17          18 \n -7.0166048  15.8946599 -13.8739373   6.0784025 -23.4746282   8.1179879 \n         19          20          21          22          23          24 \n 23.7486998 -15.6077312  15.0244703  -2.5618739   7.8961495   9.8865851 \n         25          26          27          28          29          30 \n 21.1132057  10.3083157  20.0373274   7.2543835   4.8026319 -20.1449256 \n         31          32          33          34          35          36 \n  6.8913673 -21.2923753  -6.1053401  24.3083157 -12.4239887  13.8946599 \n         37          38          39          40          41          42 \n  5.7663391   6.3794117  -3.2908856   4.3002408 -34.4239887 -15.0642650 \n         43          44          45          46          47          48 \n-15.3352533  11.8994421   5.9848849  11.4414187  -4.7045414   5.1752126 \n         49          50          51          52          53          54 \n -5.4254783   1.3526833  18.0769128 -14.2465180 -31.7093236 -16.7456165 \n         55          56          57          58          59          60 \n 18.5268614   2.6599645   5.2510909  -6.2732463  -6.7756375  30.8946599 \n         61          62          63          64          65          66 \n -3.2923753   1.6108145  16.1656482  33.3526833 -19.2923753 -16.1940755 \n         67          68          69          70          71          72 \n -6.5142136  -4.8820121   8.7043321  -9.0022581  12.6599645  -1.1530004 \n         73          74          75          76          77          78 \n 13.3035334  -2.3763284 -12.9230872  -8.2465180 -12.1988577   3.9405172 \n         79          80          81          82          83          84 \n  2.1227701 -13.6997592   3.5268614 -10.4779208   8.7219714 -38.9087405 \n         85          86          87          88          89          90 \n -7.4206961  20.7486998 -20.5142136  13.4905686  -2.6029490  -7.8152229 \n         91          92          93          94          95          96 \n  3.0340348  15.7978497  10.7978497  27.7171892   2.0721306  -1.7360520 \n         97 \n 12.5840862"
  },
  {
    "objectID": "readings/08-diagnosing-collinearity.html",
    "href": "readings/08-diagnosing-collinearity.html",
    "title": "📖 Diagnosing Collinearity",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Eigenvalues and Eigenvectors. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nCook, D. (2019). How to use a tour to check if your model suffers from multicollinearity. Personal blog."
  },
  {
    "objectID": "readings/09-pca-via-spectral-decomposition.html",
    "href": "readings/09-pca-via-spectral-decomposition.html",
    "title": "📖 Principal Components Analysis via Spectral Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Basis vectors and matrices. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and eigenvectors. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Spectral decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nWilke, C. O. (2020). PCA tidyverse style. Personal blog."
  },
  {
    "objectID": "readings/10-pca-via-svd.html",
    "href": "readings/10-pca-via-svd.html",
    "title": "📖 Principal Components Analysis via Singular Value Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Singular value decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nGundersen, G. (2018). Singular value decomposition as simply as possible.\nWang, Z. (2019). PCA and SVD explained with numpy.\nWikipdeia. (2020). Singular value decomposition."
  },
  {
    "objectID": "readings/11-biased-estimation-ridge-regression.html",
    "href": "readings/11-biased-estimation-ridge-regression.html",
    "title": "📖 Biased Estimation: Ridge Regression",
    "section": "",
    "text": "Fortmann-Roe, S. (2012). Understanding the bias-variance tradeoff.\n\n\n\nAdditional Resources\n\nCross-Validated. (2014). Why is ridge regression called “ridge”, why is it needed, and what happens when \\(\\lambda\\) goes to infinity?.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.\nStatistical Learning MOOC taught by Hastie and Tibshirani"
  },
  {
    "objectID": "readings/12-model-selection.html",
    "href": "readings/12-model-selection.html",
    "title": "📖 Model Selection",
    "section": "",
    "text": "Heinze, G., Wallisch, C., & Dunkler, D. (2018). Variable selection — A review and recommendations for the practicing statistician. Biometrical Journal. Biometrische Zeitschrift, 60(3), 431–449. https://doi.org/10.1002/bimj.201700067\n\n\n\nAdditional Resources\n\nolsrr Vignette\nWilliams, B., Hansen, G., Baraban, A., & Santoni, A. (2015). A practical approach to variable selection—A comparison of various techniques. Casualty Actuarial Society E-Forum, Summer, 1–20."
  },
  {
    "objectID": "readings/13-cross-validation.html",
    "href": "readings/13-cross-validation.html",
    "title": "📖 Cross-Validation",
    "section": "",
    "text": "Feick, L. (2019). Evaluating model performance by building cross-validation from scratch. STATWORX Blog.\n\n\n\nAdditional Resources\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). Cross-validation. In An introduction to statistical learning: with applications in R (pp. 176–186). New York: Springer."
  },
  {
    "objectID": "readings/14-regression-from-summary-measures.html",
    "href": "readings/14-regression-from-summary-measures.html",
    "title": "📖 Regression from Summary Measures",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Statistical application: SSCP, variance–covariance, and correlation matrices.. Matrix Algebra for Educational Scientists.\n\nIn particular pay attention to the relationships between these three matrices."
  }
]