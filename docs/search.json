[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Instructor/TA",
    "section": "",
    "text": "Instructor: Andrew Zieffler  Email: zief0002@umn.edu  Office: Education Sciences Building 178  Office Hours: Monday 9:00 AM–10:00 AM; and by appointment  Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link.\n\nTA: Carlos Chavez  Email: chave143@umn.edu  Office: Education Sciences Building 275  Office Hours: Wednesday 11:00 AM–1:00 PM; and by appointment"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Due Dates",
    "section": "",
    "text": "Assignment\n\n\nDue Date\n\n\nHTML\n\n\n\n\n\n\nAssignment 01: Matrix Algebra for Linear Regression \n\n\nSept. 27\n\n\n\n\n\n\n\nAssignment 02: Simulating from the Regression Model\n\n\nOct. 06\n\n\n\n\n\n\n\nAssignment 03: Regression Diagnostics\n\n\nOct. 13\n\n\n\n\n\n\n\nAssignment 04: Using WLS to Model Data with Outliers\n\n\nOct. 20\n\n\n\n\n\n\n\nAssignment 05: Collinearity and Dimension Reduction\n\n\nNov. 01\n\n\n\n\n\n\n\nAssignment 06: Ridge Regression\n\n\nNov. 15\n\n\n\n\n\n\n\nAssignment 07: Cross-Validation\n\n\nDec. 06\n\n\n\n\n\n\n\nAssignment 08: Path Analysis\n\n\nDec. 15"
  },
  {
    "objectID": "assignments/assignment-01.html",
    "href": "assignments/assignment-01.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience using matrix algebra to compute various analytic output for regression. In this assignment, you will use the data given below that includes measurements for 10 countries on: infant mortality rate per 1000 live births (infant), the per-capita income (pci) and world region (region) of the country.\n\n\n\n\n\ncountry\ninfant\npci\nregion\n\n\n\n\nAlgeria\n86.3\n400\nAfrica\n\n\nBolivia\n60.4\n200\nAmericas\n\n\nBurundi\n150.0\n68\nAfrica\n\n\nDominican Republic\n48.8\n406\nAmericas\n\n\nKenya\n55.0\n169\nAfrica\n\n\nMalawi\n148.3\n130\nAfrica\n\n\nNicaragua\n46.0\n507\nAmericas\n\n\nParaguay\n38.6\n347\nAmericas\n\n\nRwanda\n132.9\n61\nAfrica\n\n\nTrinidad & Tobago\n26.2\n732\nAmericas\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\n\n\nUnstandardized Regression\nYou will be fitting the model lm(infant ~ 1 + pci + region + pci:region). Within this model, use dummy coding to encode the region predictor and make Americas the reference group.\n\nWrite out the elements of the matrix \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the design matrix.\nDoes \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) have an inverse? Explain.\nCompute (using matrix algebra) and report the vector of coefficients, b for the OLS regression.\nCompute (using matrix algebra) and report the variance–covariance matrix of the coefficients.\nUse the values from b (Question 3) and from the variance–covariance matrix you reported in the previous question to find the 95% CI for the coefficient associated with the main-effect of PCI. (Hint: If you need to refresh yourself on how CIs are computed, see here.)\nCompute (using matrix algebra) and report the hat-matrix, H. Also show how you would use the values in the hat-matrix to find \\(\\hat{y}_1\\) (the predicted value for Algeria).\nCompute (using matrix algebra) and report the vector of residuals, e.\nCompute (using matrix algebra) and report the estimated value for the RMSE.\nGiven the assumptions of the OLS model and the RMSE estimate you computed in the previous question, compute and report the variance–covariance matrix of the residuals.\n\n\n\n\nANOVA Decomposition\nIn this section you will be re-creating the output from the ANOVA decomposition for the model fitted in the previous section.\n\nCompute (using matrix algebra) and report the model, residual, and total sum of squares terms in the ANOVA decomposition table. (2pts)\nCompute (using matrix algebra) and report the model, residual, and total degrees of freedom terms in the ANOVA decomposition table. (2pts)\nUse the values you obtained in Questions 11 and 12 to compute the model and residual mean square terms.\nUse the mean square terms you found in Question 13 to compute the F-value for the model (i.e., to test \\(H_0:\\rho^2=0\\)). Also compute the p-value associated with this F-value. (Hint: If you need to refresh yourself on how F-values or p-values are computed, see here.)\n\n\n\n\nRegression: Effects-Coding\nNow consider fitting the model to the data to examine whether there is an effect of region (no other predictors) on infant mortality. In this model, we will use effects-coding to encode the region variable (see here). This model is often expressed as:\n\\[\n\\mathrm{Infant~Mortality}_i = \\mu + \\alpha_{\\mathrm{Region}} + \\epsilon_i\n\\]\n\nWrite out the design matrix that would be used to fit this model.\nCompute (using matrix algebra) and report the vector of coefficients, b, from the OLS regression.\nCompute (using matrix algebra) and report the variance–covariance matrix for the coefficients.\nExplain why the sampling variances for the coefficients are the same and why the sampling covariance is zero by referring to computations produced in the matrix algebra. (2pts)"
  },
  {
    "objectID": "assignments/assignment-02.html",
    "href": "assignments/assignment-02.html",
    "title": "Assignment 02",
    "section": "",
    "text": "Instructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\n\n\nSimulation 1: Modeling Heteroskedasticity\nIn this simulation, you will explore what happens to the regression estimates when the assumption of homoskedasticity is violated. For this simulation use a sample size of 200.\n\nCreate the fixed X values you will use in each trial of the simulation. To do this, draw \\(n=200\\) X-values from a uniform distribution with a minimum of \\(-2\\) and a maximum of \\(+2\\). Prior to drawing these values, set your starting seed to 678910. Report the syntax you used, and the first six X values.\nCreate a the Y-values for the first trial of the simulation by using the model:\n\n\\[\n\\begin{split}\ny_i &= -3.2 + 1.75(x_i) + \\epsilon_i \\\\[2ex]\n\\epsilon_i &\\overset{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma)\n\\end{split}\n\\]\n\nwhere\n\n\\[\n\\begin{split}\n\\sigma &= e^{\\gamma(x_i)}\\\\[2ex]\ne &\\mathrm{~is~Euhler's~constant~}(\\approx2.718282) \\\\[2ex]\n\\gamma &= 1.5\n\\end{split}\n\\]\n\nHere the variation in the random error is a function of X and random noise. Report the syntax you used, and the first six Y values.\n\n\nCreate and report the scatterplot of the Y-values versus the X-values for this first trial of the simulation.\nDescribe the pattern of heteroscedasticity.\nDoes the pattern of heteroscedasticity you described in Question 4 make sense given how the error term was created. Explain.\n\nCarry out 1000 trials of the simulation. (Reminder: Be sure to use these same X values in each trial of the simulation; they are fixed.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error.\n\nCompute and report the mean value for the residual standard error.\n\n\n\n\nSimulation 2: Homoskedastic Model\nTo evaluate the different estimates from the heteroskedasticity model, we need to compare them to estimates drawn from a homoskedastic model with the same population coefficients. To make the comparisons “fair”, so that we are only evaluating the effects of the heteroskedasticity, we also need to run this simulation using a residual standard error that is equal to the mean from the heteroskedastic simulation (i.e., \\(\\mathtt{sd\\neq1}\\) in the rnorm() function).\n\nCarry out 1000 trials of the simulation for the appropriate homoskedastic model. (Reminder: Be sure to use these same X values in this simulation as in the previous simulation.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error. Report your syntax.\n\n\n\n\nComparing Results from the Two Simulations: Evaluating the Effects of Hetroskedasticity\n\nCreate a density plot of the distribution of intercept estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the intercept.\nBased on your responses to Question 8, does the intercept estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 8, does the intercept estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of slope estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the slope\nBased on your responses to Question 11, does the slope estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 11, does the slope estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of residual standard error estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the residual standard error\nBased on your responses to Question 14, does the residual standard error estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 14, does the residual standard error estimate seem to be less efficient under heteroskedasticity? Explain."
  },
  {
    "objectID": "assignments/assignment-03.html",
    "href": "assignments/assignment-03.html",
    "title": "Assignment 03",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\n\n\nExploratory Analysis\n\nStart by creating scatterplots to examine the relationship between each of the predictors and the outcome. Are there observations that look problematic in these plots? If so, identify the country(ies).\nFit the regression model (specified in the introduction) to the data. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\n\n\n\n\nOutliers, Leverage, and Influence\n\nCompute the externally studentized residuals for the observations based on the fitted regression. Based on these values, identify any countries that you would consider as regression outliers. Explain why you identified these countries as regression outliers.\nFit a mean-shift model that will allow you to test whether the observation with the largest absolute studentized residual is statistically different from zero. Report the coefficient-level output (B, SE, t, and p) for this model.\nFind (and report) the Bonferroni adjusted p-value for the observation with the largest absolute studentized residual. Based on this p-value, is there statistical evidence to call this observation a regression outlier? Explain.\nCreate and include an index plot of the leverage values. Include a line in this plot that displays the cutpoint for “high” leverage. Based on this plot, identify any countries with large leverage values.\nBased on the evidence you have looked at in Questions #4–7, do you suspect that any of the countries might influence the regression coefficients? Explain.\n\n\n\n\nInfluence Measures\n\nFor each of the influence measures listed below, create and include an index plot of the influence measure. For each plot, also include a line that displays the cutpoint for “high” influence. (2pts)\n\nScaled (standardized) DFBETA values\nCook’s D\nDFFITS\nCOVRATIO\n\nShow how the Cook’s D value for the country with the largest Cook’s D value is calculated using the country’s leverage value and standardized residual.\n\n\n\n\nRemove and Refit\n\nBased on all of the evidence from the different influence measures you examined, identify and report the country(ies) that are influential. Explain how you decided on this set of observations.\nRemove the observations you identified in Question #12 from the data and refit the regression model omitting these observations. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\nCompare and contrast the coefficient-level inferences from the model fitted with the full data and that fitted with the omitted observations.\nCompare and contrast the model-level summaries, namely \\(R^2\\) and the RMSE, from the model fitted with the full data and that fitted with the omitted observations."
  },
  {
    "objectID": "assignments/assignment-04.html",
    "href": "assignments/assignment-04.html",
    "title": "Assignment 04",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\n\n\nExploratory Analysis\n\nStart by creating a scatterplot to examine the relationship between socialist party strength and income inequality (outcome).\nAre there observations that look problematic in this plot? If so, identify the country(ies).\nFit a linear model regressing income inequality on socialist party strength. Examine and report a set of regression diagnostics that allow you to identify any observations that are regression outliers.\n\n\n\n\nWeighted Least Squares Estimation\nRather than removing regression outliers from the data, we can instead fit a model that accounts for these observations. For example, fitting a model that allows for higher variance at \\(x\\)-values that have outliers. With higher variances, we would expect more extreme observations because of the increased variance. The WLS model allows for heteroskedasticity and can be used to model data that have extreme observations.\n\nCompute the empirical weights that you will use in the WLS estimation. Report the weight for the United States. (Hint: We do not know the true variances in the population.)\nFit the WLS model. Report the fitted equation.\nBased on the model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nCreate a scatterplot that shows the relationship between socialist party strength and income inequality. Include the country names as labels (or instead of the points). Include both the OLS and WLS regression lines on this plot.\nBased on the plot, comment on how the residuals from the WLS model compare to the residuals from the OLS model.\nBased on your response to Question #8, how will the model-level \\(R^2\\) value from the WLS model compare to the model-level \\(R^2\\) from the OLS model. Explain.\nThe mathematical formulaa for computing the studentized residuals for both the OLS and WLS models is given below. Compute and report the studentized residuals, using this formula, from both the OLS and WLS models for any regression outliers you identified in Question #2. (Hint: Remember that in an OLS regression the weight is 1 for each observation.)\n\n\\[\ne^{\\prime}_i = \\frac{e_i}{s_{e(-i)}\\sqrt{1-h_{ii}}} \\times \\sqrt{w_i}\n\\]\n\nBased on the values of the studentized residuals in the WLS model, are the observations you identified as regression outliers from the OLS model still regression outliers in the WLS model? Why or why not?\nExplain why the is the case by referring to the formula.\nCreate and report residual plots of the studentized residuals versus the fitted values for the OLS and WLS models. Comment on which model better fits the assumptions.\n\n\n\n\nIncluding Covariates\nNow include the energy covariate into the model to examine the effect of socialist strength after controlling for economic development. Since the model has changed, we need to re-compute the weights and re-carry out the WLS analysis.\n\nUse matrix algebra to compute the empirical weights based on the two-predictor model and report the weight for the United States. (That is use matrix algebra to carry out the steps in the 5-step process of computing weights when error variances are unknown for the WLS.)\nFit the two-predictor WLS model using matrix algebra. Report the fitted equation.\nCompute and report the standard errors of the two-predictor WLS model using matrix algebra.\nUsing your results from Questions #14 and #15, compute and report the t-values and p-values. While you can use the output of the tidy(), summary(), or other functions that automatically compute p-values to check your work, use the pt() function to answer this question. (Show your work or syntax for full credit.)\nBased on the two-predictor WLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nBased on the two-predictor OLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nWhich set of the model results should we trust. Explain by referring to the tenability of the assumptions."
  },
  {
    "objectID": "assignments/assignment-05.html",
    "href": "assignments/assignment-05.html",
    "title": "Assignment 05",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\n\nIn questions that ask you to “use matrix algebra” to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 18 points.\n\n\n\nExploratory Analysis\n\nCompute and report the correlation matrix of the 12 predictors.\nBased on the correlations, comment on whether there may be any potential collinearity problems. Explain.\nCompute and report the eigenvalues for the correlation matrix you created in Question 1.\nBased on the eigenvalues, comment on whether there may be any potential collinearity problems. Explain.\n\n\n\n\nInitial Model\nSince the ratings are assigned based on different numbers of attorneys, use a weight equal to the number of respondents to fit a WLS model that regresses the standardized retention percentage on the 12 standardized predictors.\n\nReport the coefficient-level output, including the estimated coefficients (beta weights), standard errors, t-values, and p-values.\nBased on the VIF values, comment on whether there may be any potential collinearity problems. Explain.\nUsing the predictor with the largest VIF value, use the VIF value to indicate how the standard error for this predictor will be impacted by the collinearity.\n\n\n\n\nPrincipal Components Analysis\nIn this section you are going to carry out the principal components analysis by using singular value decomposition on the correlation matrix of the predictors.\n\nCompute the composite score based on the first principal component for the first observation (Judge John J. Bauercamper). Show your work in an equation.\n\nRead the section on scree plots (Section 4) in this web article.\n\nCreate a scree plot showing the eigenvalues for the 12 principal components from the previous analysis.\nUsing the “elbow criterion”, how many principal components are sufficient to describe the data? Explain by referring to your scree plot.\nUsing the “Kaiser criterion”, how many principal components are sufficient to describe the data? Explain.\nUsing the “80% proportion of variance criterion”, how many principal components are sufficient to describe the data? Explain.\n\n\n\n\nRevisit the Regression Analysis\nThe evidence from the previous section suggests that the first two principal components are sufficient to explain the variation in the predictor space.\n\nBy examining the pattern of correlations (size and directions) in the first two principal components, identify the construct defined by the composites of these two components. Explain.\nFit the regression analysis using the first two principal components as predictors of retention percentage. (Don’t forget your weights.) Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?\nAgain, fit the same regression model using the first two principal components as predictors of retention percentage, but this time also include a quadratic effect of the first principal component. Create and report the plot of the residuals vs. fitted values. What does this suggest about the validity of the linearity assumption?\nInterpret the quadratic effect of the first principal component from this model. (It may be helpful to create a plot of the effect to guide your interpretation.)\n\n\n\n\nInfluential Values\n\nBased on Cook’s D, identify the name of any judges (and their Cook’s D value) that are influential observations.\nRemove any influential observations identified in Question 17. Re-fit the same model. Based on comparing the model- and coefficient-level output for this model and the model which included all the observations, comment on how these observations were influencing the \\(R^2\\) value, the estimate of the quadratic effect of PC1, and the effect of PC2."
  },
  {
    "objectID": "assignments/assignment-06.html",
    "href": "assignments/assignment-06.html",
    "title": "Assignment 06",
    "section": "",
    "text": "The goal of the analysis you are going to undertake in this assignment is to build a model that predicts customers’ credit card balance. To do this you will use the data provided in the file credit.csv. The six predictors included in the dataset have all been previously shown to predict credit card balance."
  },
  {
    "objectID": "assignments/assignment-06.html#model-level-summaries",
    "href": "assignments/assignment-06.html#model-level-summaries",
    "title": "Assignment 06",
    "section": "Model-Level Summaries",
    "text": "Model-Level Summaries\n\nCompute and report the model-level \\(R^2\\) for the ridge regression model. (Hint: Remember that the model-level \\(R^2\\) is the squared correlation between the observed and predicted values of the outcome.) Show your work. How does this compare to the \\(R^2\\) from the OLS model?\nCompute and report the F-value associated with the \\(R^2\\) value you computed in Question #18.\nCompute and report the p-value associated with the test of whether \\(\\rho^2=0\\)."
  },
  {
    "objectID": "assignments/assignment-07.html",
    "href": "assignments/assignment-07.html",
    "title": "Assignment 07",
    "section": "",
    "text": "Part I: Minneapolis Violent Crime\nFor the first part of this assignment, you will use the data provided in the file mpls-violent-crime.csv to build a model that examines the trend in violent crime rate over time.\n\n[CSV]\n[Data Codebook]\n\n\n\nPreparation\nCreate a variable that indicates the number of years since 2000. Use this variable in all analyses for Part I rather than the year variable.\n\n\n\nDescription\n\nCreate a scatterplot showing the violent crime rate as a function of time.\nBased on the plot, describe the trend in violent crime rate over time.\nIf you were going to fit a polynomial model to these data, what degree polynomial would you fit? Explain.\n\n\n\n\nUse p-Value methods for Model Selection\n\nFit a series of polynomial models starting with a linear model, and then models that also include higher order polynomials that allow you to evaluate your response to Question #3. Be sure to fit models up to degree \\(k+1\\), where \\(k\\) is the degree you hypothesized in Question #3. Analyze each of the polynomial terms (including the linear term) by using a series of nested F-tests. Report these results in an ANOVA table. (Note: If you need a refresher on fitting polynomial models and carrying out a nested F-test, see the Polynomial Regression notes from EPsy 8252.)\nBased on these results, which polynomial model would you adopt? Explain.\n\n\n\n\nUsing LOOCV for Model Selection\nIn this section of the assignment, you are going to use LOOCV to evaluate the MSE for the same set of polynomial models you evaluated in Question #4.\n\nWrite and include syntax that will carry out the LOOCV.\nReport the cross-validated MSE for each of the models in your set of polynomial models.\nBased on these results, which degree polynomial model should be adopted? Explain.\n\n\n\n\n\nPart II: Course Evaluations\nFor the second part of this assignment, you will use the data provided in the file evaluations.csv to build a model that predicts variation in course evaluation scores.\n\n[CSV]\n[Data Codebook]\n\n\n\nPreparation\nBegin by fitting a model that predicts average course evaluation score using the following predictors: beauty, number of courses for which the professor has evaluations, whether the professor is a native English speaker, and whether the professor is female.\n\n\n\nDescription\n\nUsing average course evaluation scores (y), compute the total sum of squares (SST). Show your work.\nUsing average course evaluation scores (y) and the predicted values from the model (\\(\\hat{y}\\)), compute the sum of squared errors (SSE). Show your work.\nCompute the model \\(R^2\\) value using the formula: \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\).\n\n\n\n\nUsing k-Fold Cross-Validation to Estimate \\(R^2\\)\nAs we know, the estimate for \\(R^2\\) is biased. We can obtain a better estimate of \\(R^2\\) by using cross-validation. You will use 5-fold cross-validation to estimate the \\(R^2\\) value. The algorithm for this will be:\n\nRandomly divide the evaluations data into 5 folds.\nHold out 1 fold as your validation data and use the remaining 4 folds as your training data.\n\nFit the model to the training data.\nUse the estimated coefficients from those fits to compute \\(\\hat{y}\\) values using the validation data.\nCompute the SST and SSE values for the validation data, and use those to compute \\(R^2\\) based on the formula \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\). (Note that sometimes the \\(R^2\\) may be negative when we compte it in this manner.)\n\nRepeat for each fold.\nCompute the cross-validated \\(R^2\\) by finding the mean of the five \\(R^2\\) from the cross-validations.\n\n\nWrite and include syntax that will carry out the 5-fold cross-validation. In this syntax use set.seed(1000) so that you and the answer key will get the same results. (This website may be useful in using the purrr package to obtain the y- and \\(\\hat{y}\\)-values in order to compute the SST and SSE values: https://drsimonj.svbtle.com/k-fold-cross-validation-with-modelr-and-broom)\nReport the five \\(R^2\\) values from your analysis and the cross-validated \\(R^2\\) value.\nHow does this value compare to the \\(R^2\\) value you computed in Question #11, based on the data.\nExplain why the cross-validated estimate of \\(R^2\\) is a better estimate than the data-based \\(R^2\\).\n\n\n\n\n\nPart III: Credit Balance\nFor the third part of this assignment, you will again use the file the data provided in the file credit.csv to build a model that predicts customers’ credit card balance. Do not forget to standardize all the variables.\n\n[CSV]\n[Data Codebook]\n\n\n\nUse the lm.ridge() function to fit the same sequence of \\(\\lambda\\) values you used in Question #7 from Assignment 6. Running select() on this output, provides \\(\\lambda\\) values based on different criteria. Report the \\(\\lambda\\) value associated with the generalized cross-validation (GCV) metric.\nRe-do Question #7 from Assignment 6, except this time, select the optimal \\(\\lambda\\) value based on using the AICc. How does this compare to the \\(\\lambda\\) value you found using the GCV metric from the previous question? (Show your syntax.)\nCompute the coefficients, standard errors based on the ridge regression model based on the \\(\\lambda\\) value you identified in Question #17. Also compute the t-values, and p-values for each coefficient. Report all of these in a coefficient table."
  },
  {
    "objectID": "assignments/assignment-08.html",
    "href": "assignments/assignment-08.html",
    "title": "Assignment 08",
    "section": "",
    "text": "Background\nTaimalu and Luik (2021)1 used a path model to examine four research questions about “how perceived knowledge about technology integration, and beliefs and attitudes towards using technology, impact the intention to use technology among student teachers” (p. 1). Their model, which is based on the relevant literature is shown here:\n\n\n\n\n\nThey hypothesized that all the path coefficients would be positive. A questionnaire consisting of Likert-scale items related to six constructs was administered to 232 student teachers from the University of Tartu. These constructs, identified in the literature as influencing use of technology, include:\n\nPerceived knowledge about technology integration. These items measure the technological pedagogical knowledge (TPK), technological content knowledge (TCK), and technological pedagogical and content knowledge (TPACK).\nPerceived ease of use (e.g., I find computers easy to use; computer icons are easy to understand for me)\nPerceived usefulness of technology for students (e.g., using technology improves cooperation between the learners);\nPerceived usefulness of technology for teachers (e.g., using technology enhances my effectiveness);\nAttitude toward using technology in education (e.g., technology is valuable in teaching; every teacher must be able to use technology); and\nIntention to use (e.g., I intend to allow learners to use the technology to explore different topics; I intend to guide students to use the Internet to communicate with experts or other learners to enrich their learning experiences).\n\nComposites (using a standardized scale) were created from the items for each construct and the correlation matrix fro these composites (shown below) was used to fit the proposed path model.\n\n\n\n\nIntercorrelations between six measures related to technology integration collected from student teachers. All measures were standardized to have *M* = 0 and *SD* = 1.\n \n  \n      \n    1 \n    2 \n    3 \n    4 \n    5 \n    6 \n  \n \n\n  \n    1. Intention to use \n    --- \n     \n     \n     \n     \n     \n  \n  \n    2. Perceived knowledge about technology integration \n    0.219 \n    --- \n     \n     \n     \n     \n  \n  \n    3. Perceived ease of use \n    0.33 \n    0.669 \n    --- \n     \n     \n     \n  \n  \n    4. Perceived usefulness for students \n    0.521 \n    0.244 \n    0.364 \n    --- \n     \n     \n  \n  \n    5. Perceived usefulness for teachers \n    0.454 \n    0.309 \n    0.462 \n    0.56 \n    --- \n     \n  \n  \n    6. Attitude toward using \n    0.696 \n    0.181 \n    0.27 \n    0.699 \n    0.53 \n    --- \n  \n\n\n\n\n\n\n\n\nPart 1: Regression from Summary Values\nFor the questions in this section, use the summary values provided to fit a regression to predict variation in intention to use (Model A). Include the predictors of intention to use declared in the path model. Show your syntax for each of these questions.\n\nCompute the standardized regression coefficients for Model A.\nCompute the \\(R^2\\), adjusted-\\(R^2\\) values for Model A.\nCompute the estimated residual standard error for Model A.\nCompute the standard errors for the regression coefficients for Model A.\n\n\n\n\nPart 2: Simulate Data\n\nSimulate the study’s data based on the summary values provided. Assume multivariate normality among the measures. Show the first six rows of the simulated data. Also include your syntax. For reproducibility, set the random starting seed to 57.\n\n\n\n\nPart 3: Path Analysis: All Hypothesized Paths\n\nUse the data you simulated in Part 2 to fit all the relevant regression models in the path analysis. Re-draw the path model and include all the estimated path coefficients, 95% confidence intervals associated with these coefficients, and all error terms/disturbances in the model. (3pts)\n\n\n\n\nPart 4: Path Analysis: Empirically Supported Paths\n\nUse the confidence intervals associated with the path coefficients to evaluate the researchers’ hypotheses that all the effects are positive. Based on these evaluations, re-draw the path model. Omit any paths for hypotheses in the model that are not empirically supported. (2pts)\nCreate a publishable quality table that includes the direct, indirect, and total effects for all of the potential causes of intended technology use in the updated path model. (2pts)\nConsider the following research question: How does student teachers’ perceived ease of use of technology influence their intention to use technology in education?? Use the results from the updated path model to provide an answer to this question. Your response, written in prose (a few sentences), should clearly provide an answer to this question by drawing on the relevant effects in the updated path model. (2pts)\n\n\n\n\n\n\nFootnotes\n\n\nLuik, P., & Taimalu, M. (2021). Predicting the intention to use technology in education among student teachers: A path analysis. Education Sciences, 11(9), 564. https://doi.org/10.3390/educsci11090564↩︎"
  },
  {
    "objectID": "codebooks/contraception.html",
    "href": "codebooks/contraception.html",
    "title": "contraception.csv",
    "section": "",
    "text": "The data in contraception.csv were collected from several sources (e.g., World Bank). The variables are:\n\ncountry: Country name\nregion: Region of the world\ncontraceptive: Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.\neduc_female: Average number of years of formal education (schooling) for females\ngni: Categorical measure of the economy indicating if the country has a low or high gross national income\n\n\n\nPreview\n\n\nCode\n# Import data\ncontraception = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/contraception.csv\")\n\n# View data\ncontraception\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nReferences\n\n\nRoser, M. (2017). Fertility rate. Our world in data.\n\n\nUNICEF. (2016). State of the world’s children 2016. United Nations Population Division’s World Contraceptive Use, household surveys including Demographic and Health Surveys and Multiple Indicator Cluster Surveys.\n\n\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/credit.html",
    "href": "codebooks/credit.html",
    "title": "credit.csv",
    "section": "",
    "text": "The data in credit.csv contains simulated credit card data for 400 individuals. These data are from James et al. (2013). The variables in the data set are:\n\nbalance: Customer’s average credit card balance (in dollars)\nincome: Customer’s reported income (in $10,000 dollars)\nlimit: Credit limit issued to customer\nrating: Customer’s credit rating; higher values indicate a better credit rating\ncards: Number of credit cards the customer has\nage: Customer’s age\neducation: Number of years of education\n\n\n\nPreview\n\n\nCode\n# Import data\ncredit = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/credit.csv\")\n\n# View data\ncredit\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: With applications in R. Springer."
  },
  {
    "objectID": "codebooks/equal-education-opportunity.html",
    "href": "codebooks/equal-education-opportunity.html",
    "title": "equal-education-opportunity.csv",
    "section": "",
    "text": "In 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). The data in equal-education-opportunity.csv are a subset of the data collected. These data, provided by Chatterjee & Hadi (2012), constitute a random sample of 70 schools from the original data. The attributes, which have all been mean-centered and standardized, include:\n\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty’s credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n\nPreview\n\n\nCode\n# Import data\neeo = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/equal-education-opportunity.csv\")\n\n# View data\neeo\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House."
  },
  {
    "objectID": "codebooks/evaluations.html",
    "href": "codebooks/evaluations.html",
    "title": "evaluations.csv",
    "section": "",
    "text": "The data in evaluations.csv come from Hamermesh & Parker (2005) and were made available by Gelman & Hill (2007). This data were collected from student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations. The variables are:\n\nprof_id: Professor ID number\navg_eval: Average course rating\nnum_courses: Number of courses for which the professor has evaluations\nnum_students: Number of students enrolled in the professor’s courses\nperc_evaluating: Average percentage of enrolled students who completed an evaluation\nbeauty: Measure of the professor’s beauty composed of the average score on six standardized beauty ratings\ntenured: Is the professor tenured? (0 = non-tenured; 1 = tenured)\nnative_english: Is the professor a native English speaker? (0 = non-native English speaker; 1 = native English speaker)\nage: Professor’s age (in years)\nfemale: Is the professor female? (0 = not female; 1 = female)\n\n\n\nPreview\n\n\nCode\n# Import data\nevaluations = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/evaluations.csv\")\n\n# View data\nevaluations\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press.\n\n\nHamermesh, D. S., & Parker, A. M. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24, 369–376."
  },
  {
    "objectID": "codebooks/iowa-judges.html",
    "href": "codebooks/iowa-judges.html",
    "title": "iowa-judges.csv",
    "section": "",
    "text": "The Iowa State Bar Association has conducted the Judicial Performance Review as a way of giving voters information on the Iowa judges up for retention in an election year. Judges are evaluated on their professional competence and demeanor as determined by the attorneys who frequently appeared before them. The attorneys also indicate whether or not they believe the judge should be retained. The data in iowa-judges.csv were published by the The Iowa State Bar Association (2018). The variables are:\n\nyear: Year of the judicial performance review\njudge: Name of the judge\ndistrict: Judicial district\nrespondents: Number of attorneys who rated the judge\nretention: Percentage of attorneys who indicated the judge should be retained\n\nThe following characteristics were rated on the scale: 5 (Excellent; performance is outstanding), 4 (Good; performance is above average), 3 (Satisfactory; performance is adequate), 2 (Deficient; performance is below average), 1 (Very Poor; performance is well below average and unacceptable).\n\nknowledge: Knowledge and application of the law\nperception: Perception of factual issues\npunctuality: Punctuality for court proceedings\nattention: Attentiveness to evidence and arguments\nmanagement: Management and control of the courtroom\ndemeanor: Temperament and demeanor\nclarity: Clarity and quality of written opinions\npromptness: Promptness of rulings and decisions\n\nThe following characteristics were rated on the scale: 5 (Strongly Agree), 4 (Agree), 3 (Neither), 2 (Disagree), 1 (Strongly Disagree)\n\ncriticism: Avoids undue personal observations or criticisms of litigants, judges and lawyers from bench or in written opinions\ndecision: Decides cases on basis of applicable law and fact, not affected by outside influence.\ncourteous: Is courteous and patient with litigants, lawyers and court personnel.\nequality: Treats people equally regardless of race, gender, age, national origin, religion, sexual orientation, socio-economic status or disability.\n\n\n\nPreview\n\n\nCode\n# Import data\njudges = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/iowa-judges.csv\")\n\n# View data\njudges\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nThe Iowa State Bar Association. (2018). 2018 judicial performance review. The Iowa State Bar Association. https://cdn.ymaws.com/www.iowabar.org/resource/resmgr/judicial_performance_review/2018/2018_Judicial_Performance_Re.pdf"
  },
  {
    "objectID": "codebooks/mpls-violent-crime.html",
    "href": "codebooks/mpls-violent-crime.html",
    "title": "mpls-violent-crimes.csv",
    "section": "",
    "text": "The mpls-violent-crime.csv file contains data collected from the Minneapolis Police Department and reported by the Star Tribune on The two attributes in this file are:\n\nyear: Year\ncrime_rate: Violent crime rate per 100,000 people\n\n\n\nPreview\n\n\nCode\n# Import data\ncrime = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/mpls-violent-crime.csv\")\n\n# View data\ncrime"
  },
  {
    "objectID": "codebooks/slid.html",
    "href": "codebooks/slid.html",
    "title": "slid.csv",
    "section": "",
    "text": "The data in slid.csv includes a subset of data collected during the 1994 wave of Statistics Canada’s Survey of Labour and Income Dynamics (SLID). These data constitute employed citizens living in Ontario, Canada between the ages of 16 and 65. These data are taken from the public-use dataset made available by Statistics Canada, and prepared by the Institute for Social Research, York University. They were made available in Fox et al. (2022). The variables in the data set are:\n\nwages: Composite hourly wage rate based on all the participant’s jobs\nage: Age of the participant (in years)\neducation: Number of years of schooling\nmale: A dummy-coded predictor for sex (0=Non-male; 1=Male)\n\n\n\nPreview\n\n\nCode\n# Import data\nslid = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/slid.csv\")\n\n# View data\nslid\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFox, J., Weisberg, S., & Price, B. (2022). CarData: Companion to applied regression data sets. R package version 3.0-5. https://CRAN.R-project.org/package=carData"
  },
  {
    "objectID": "codebooks/stack-1979.html",
    "href": "codebooks/stack-1979.html",
    "title": "stack-1979.csv",
    "section": "",
    "text": "Stack (1979) studied predictors of income inequality in a paper published in the American Sociological Review. He posited that more political participation and a strong Socialist party in a country would be associated with less income inequality. To control for variation in economic development, he included a measure of energy consumption, which had been identified as a reasonable economic proxy in previous studies. Stack’s data, stored in stack-1979.csv, include four attributes measured on 18 countries. The attributes are:\n\ncountry: Country name\ninequality: Ratio of the share of income received by the most wealthy population quintile (richest 20%) to the share received by the poorest 40% of the population; Higher values indicate more income inequality\nturnout: Proportion of the adult population voting in the most recent national election prior to 1972\nenergy: Energy consumption per capita (expressed in million metric tons of coal equivalents; higher values indicate more economic development\nsocialist: Annual average proportion of seats held by socialist parties in the national legislature, over the first twenty postwar years\n\n\n\nData Preview\n\n\nCode\n# Import data\nstack = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/stack-1979.csv\")\n\n# View data\nstack\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nStack, S. (1979). The effects of political participation and socialist party strength on the degree of income inequality. American Sociological Review, 44(1), 168–171."
  },
  {
    "objectID": "codebooks/states-2019.html",
    "href": "codebooks/states-2019.html",
    "title": "states-2019.csv",
    "section": "",
    "text": "The data in states-2019.csv include statistics collected from Wikipedia, the 2019 American Community Survey, and the National Centers for Environmental Information. The attributes in the data are:\n\nstate: State/territory name\nlife_expectancy: Life expectancy (in years)\npopulation: Population estimate (in millions)\nincome: Per capita income (in thousands of dollars)\nilliteracy: Illiteracy rate (in percent)\nmurder: Murder and non-negligent manslaughter rate (per 100,000 population)\nhs_grad: Percentage of high school graduates\nfrost: Mean number of days with minimum temperature below freezing\narea: Land area (in square miles)\n\n\n\nPreview\n\n\nCode\n# Import data\nusa = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/pensive-giraffe/main/data/states-2019.csv\")\n\n# View data\nusa\n\n\n\n\n  \n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Name\n\n\nData\n\n\nCodebook\n\n\n\n\n\n\ncontraception.csv\n\n\n  \n\n\n  \n\n\n\n\ncredit.csv\n\n\n  \n\n\n  \n\n\n\n\nequal-education-opportunity.csv\n\n\n  \n\n\n  \n\n\n\n\nevaluations.csv\n\n\n  \n\n\n  \n\n\n\n\niowa-judges.csv\n\n\n  \n\n\n  \n\n\n\n\nmpls-violent-crime.csv\n\n\n  \n\n\n  \n\n\n\n\nslid.csv\n\n\n  \n\n\n  \n\n\n\n\nstack-1979.csv\n\n\n  \n\n\n  \n\n\n\n\nstates-2019.csv"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html",
    "href": "handouts/handout-regression-example-in-practice.html",
    "title": "A Regression Example",
    "section": "",
    "text": "In this document we will use the data in contraception.csv to examine whether female education level explains variation in contraceptive useage after controlling for GNI."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#examine-the-data",
    "href": "handouts/handout-regression-example-in-practice.html#examine-the-data",
    "title": "A Regression Example",
    "section": "Examine the Data",
    "text": "Examine the Data\nWe need to correctly specify the model. Since we have no theory to guide us, this is done empirically by looking at the data.\n\n# Create scatterplot\np = ggplot(data = contraception, aes(x = educ_female, y = contraceptive)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) \n\n# Add marginal density plots\nggMarginal(p, type = \"density\")\n\n# Condition the relationship on GNI\nggplot(data = contraception, aes(x = educ_female, y = contraceptive, color = gni)) + \n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) +\n   theme_bw() +\n   labs(\n      x = \"Female education level\",\n      y = \"Contraceptive useage\"\n   ) +\n   facet_wrap(~gni)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShould we include main-effects only? Or an interaction?\nIs there non-linearity to account for (e.g., transformations)? Or does it look linear?"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "href": "handouts/handout-regression-example-in-practice.html#use-matrix-algebra-to-compute-coefficient-estimates",
    "title": "A Regression Example",
    "section": "Use Matrix Algebra to Compute Coefficient Estimates",
    "text": "Use Matrix Algebra to Compute Coefficient Estimates\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\intercal\\mathbf{X})^{-1} \\mathbf{X}^\\intercal\\mathbf{y}\n\\]\n\n# Store values\nn = nrow(contraception) #Sample size\nk = 2 #Number of predictors\n\n\n# Create outcome vector\ny = contraception$contraceptive\n\n# Create dummy variable for GNI\ncontraception = contraception %>%\n   mutate(\n      high_gni = if_else(gni == \"High\", 1, 0)\n      )\n\n# Create design matrix\nX = matrix(\n   data = c(rep(1, n), contraception$educ_female, contraception$high_gni),\n   ncol = 3\n)\n\n# Compute b vector\nb = solve(t(X) %*% X) %*% t(X) %*% y\nb\n\n          [,1]\n[1,] 27.021387\n[2,]  4.088735\n[3,]  1.608766\n\n\nThus the fitted regression equation is:\n\\[\n\\widehat{\\mathrm{Contraceptive~Use}}_i = 27.02 + 4.09(\\mathrm{Female~Education~Level}_i) + 1.60(\\mathrm{High~GNI}_i)\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#compute-residual-standard-error",
    "href": "handouts/handout-regression-example-in-practice.html#compute-residual-standard-error",
    "title": "A Regression Example",
    "section": "Compute Residual Standard Error",
    "text": "Compute Residual Standard Error\n\n# Compute e vector\ne = y - X %*% b\n\n# Compute s_e\ns_e = sqrt((t(e) %*% e) / (n - k - 1))\ns_e\n\n         [,1]\n[1,] 14.39792\n\n\nThus the residual standard error (a.k.a., the root mean square error; RMSE) is:\n\\[\ns_e = 14.40\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#compute-variancecovariance-matrix-for-the-coefficients",
    "href": "handouts/handout-regression-example-in-practice.html#compute-variancecovariance-matrix-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Compute Variance–Covariance Matrix for the Coefficients",
    "text": "Compute Variance–Covariance Matrix for the Coefficients\n\\[\n\\mathrm{Var}(\\mathbf{b}) = s^2_e(\\mathbf{X}^\\intercal\\mathbf{X})^{-1}\n\\]\nwhere \\(s^2_e = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n-k-1}\\)\n\n# Compute varaince-covariance matrix of b\nV = as.numeric(s_e^2) * solve(t(X) %*% X)\nV\n\n          [,1]       [,2]      [,3]\n[1,] 12.414688 -1.8783934  4.782603\n[2,] -1.878393  0.4267136 -2.028306\n[3,]  4.782603 -2.0283060 18.197825\n\n# Compute SEs for b\nsqrt(diag(V))\n\n[1] 3.5234483 0.6532332 4.2658909\n\n\nThus\n\\[\n\\mathrm{SE}(b_0) = 3.52 \\qquad \\mathrm{SE}(b_1) = 0.65 \\qquad \\mathrm{SE}(b_2) = 4.27\n\\]"
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#coefficient-level-inference",
    "href": "handouts/handout-regression-example-in-practice.html#coefficient-level-inference",
    "title": "A Regression Example",
    "section": "Coefficient-Level Inference",
    "text": "Coefficient-Level Inference\nHere we will focus on the effects of female education level since it is our focal predictor. (GNI is a control.) Note this is the second effect in the b vector and in the V matrix. We will test the hypothesis:\n\\[\nH_0: \\beta_{\\mathrm{Education}} = 0\n\\]\n\n# Compute t-value\nt_0 = (b[2] - 0) / sqrt(V[2, 2])\nt_0\n\n[1] 6.259228\n\n# Evaluate t-value\ndf = n - k - 1\np = 2* (1 - pt(abs(t_0), df = df))\np\n\n[1] 0.00000001143799\n\n\nHere,\n\\[\nt(94) = 6.26,~p=0.0000000114\n\\]\nThe evidence suggests that the data are not very compatible with the hypothesis that there is no effect of female education level on contraceptive useage, after controlling for differences in GNI."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "href": "handouts/handout-regression-example-in-practice.html#statistical-inference-confidence-intervals-for-the-coefficients",
    "title": "A Regression Example",
    "section": "Statistical Inference: Confidence Intervals for the Coefficients",
    "text": "Statistical Inference: Confidence Intervals for the Coefficients\nFrom the hypothesis test, we believe there is an effect of female education level on contraceptive useage, after controlling for differences in GNI. What is that effect? To answer this we will compute a 95% CI for the effect of female education.\n\n# Compute critical value\nt_star = qt(.025, df = df)\n\n# Compute CI\nb[2] - abs(t_star) * sqrt(V[2, 2])\n\n[1] 2.791725\n\nb[2] + abs(t_star) * sqrt(V[2, 2])\n\n[1] 5.385745\n\n\nThe 95% CI indicates that the population effect of female education level on contraceptive useage, after controlling for differences in GNI is between 2.79 and 5.39."
  },
  {
    "objectID": "handouts/handout-regression-example-in-practice.html#accessing-regression-matrices-from-lm",
    "href": "handouts/handout-regression-example-in-practice.html#accessing-regression-matrices-from-lm",
    "title": "A Regression Example",
    "section": "Accessing Regression Matrices from lm()",
    "text": "Accessing Regression Matrices from lm()\nThere are several built-in R functions that allow you to access different regression matrices once you have fitted a model with lm().\n\n# Access design matrix\nmodel.matrix(lm.1)\n\n   (Intercept) educ_female gniLow\n1            1         5.9      0\n2            1         8.9      0\n3            1        10.5      0\n4            1         4.6      1\n:            :          :       :\n97           1         6.7      1\nattr(,\"assign\")\n[1] 0 1 2\nattr(,\"contrasts\")\nattr(,\"contrasts\")$gni\n[1] \"contr.treatment\"\nThe design matrix is given and information about this design matrix is also encoded. There is an attribute “assign”, an integer vector with an entry for each column in the matrix giving the term in the formula which gave rise to the column. Value 0 corresponds to the intercept (if any), and positive values to terms in the order given by the term.labels attribute of the terms structure corresponding to object. There is also an attribute called “contrasts” that identifies any factors (categorical variables) in the model and indicates how the contrast testing (comparison of the factor levels) will be carried out. Here “contr.treatment” is used. This compares each level of the factor to the baseline (which is how dummy coding works).\n\n# Access coefficient estimates\ncoef(lm.1)\n\n(Intercept) educ_female      gniLow \n  28.630153    4.088735   -1.608766 \n\n# Access variance-covariance matrix for b\nvcov(lm.1)\n\n            (Intercept) educ_female     gniLow\n(Intercept)   40.177719  -3.9066994 -22.980428\neduc_female   -3.906699   0.4267136   2.028306\ngniLow       -22.980428   2.0283060  18.197825\n\n# Access fitted values\nfitted(lm.1)\n\n       1        2        3        4        5        6        7        8 \n52.75369 65.01990 71.56187 45.82957 71.56187 66.24652 35.19886 61.36676 \n       9       10       11       12       13       14       15       16 \n58.06905 64.20215 71.97075 34.78998 36.01660 40.10534 47.87394 78.92160 \n      17       18       19       20       21       22       23       24 \n29.47463 67.88201 57.25130 35.60773 62.97553 71.56187 78.10385 60.11341 \n      25       26       27       28       29       30       31       32 \n58.88679 48.69168 51.96267 32.74562 73.19737 51.14493 69.10863 30.29238 \n      33       34       35       36       37       38       39       40 \n40.10534 48.69168 74.42399 40.10534 55.23366 46.62059 68.29089 68.69976 \n      41       42       43       44       45       46       47       48 \n74.42399 67.06427 70.33525 49.10056 74.01512 42.55858 59.70454 54.82479 \n      49       50       51       52       53       54       55       56 \n36.42548 46.64732 40.92309 66.24652 50.70932 32.74562 67.47314 61.34004 \n      57       58       59       60       61       62       63       64 \n61.74891 66.27325 61.77564 40.10534 30.29238 54.38919 36.83435 46.64732 \n      65       66       67       68       69       70       71       72 \n30.29238 44.19408 40.51421 67.88201 59.29567 63.00226 61.34004 71.15300 \n      73       74       75       76       77       78       79       80 \n39.69647 43.37633 40.92309 66.24652 35.19886 76.05948 76.87723 68.69976 \n      81       82       83       84       85       86       87       88 \n67.47314 58.47792 57.27803 67.90874 45.42070 57.25130 40.51421 49.50943 \n      89       90       91       92       93       94       95       96 \n44.60295 72.81522 80.96597 64.20215 64.20215 48.28281 31.92787 50.73605 \n      97 \n54.41591 \n\n# Access raw residuals\nresid(lm.1)\n\n          1           2           3           4           5           6 \n  4.2463087   0.9801026 -16.5618739  16.1704304  -4.5618739 -15.2465180 \n          7           8           9          10          11          12 \n-19.1988577   5.6332361 -12.0690473 -11.2021503  -2.9707474  -2.7899842 \n         13          14          15          16          17          18 \n -7.0166048  15.8946599 -13.8739373   6.0784025 -23.4746282   8.1179879 \n         19          20          21          22          23          24 \n 23.7486998 -15.6077312  15.0244703  -2.5618739   7.8961495   9.8865851 \n         25          26          27          28          29          30 \n 21.1132057  10.3083157  20.0373274   7.2543835   4.8026319 -20.1449256 \n         31          32          33          34          35          36 \n  6.8913673 -21.2923753  -6.1053401  24.3083157 -12.4239887  13.8946599 \n         37          38          39          40          41          42 \n  5.7663391   6.3794117  -3.2908856   4.3002408 -34.4239887 -15.0642650 \n         43          44          45          46          47          48 \n-15.3352533  11.8994421   5.9848849  11.4414187  -4.7045414   5.1752126 \n         49          50          51          52          53          54 \n -5.4254783   1.3526833  18.0769128 -14.2465180 -31.7093236 -16.7456165 \n         55          56          57          58          59          60 \n 18.5268614   2.6599645   5.2510909  -6.2732463  -6.7756375  30.8946599 \n         61          62          63          64          65          66 \n -3.2923753   1.6108145  16.1656482  33.3526833 -19.2923753 -16.1940755 \n         67          68          69          70          71          72 \n -6.5142136  -4.8820121   8.7043321  -9.0022581  12.6599645  -1.1530004 \n         73          74          75          76          77          78 \n 13.3035334  -2.3763284 -12.9230872  -8.2465180 -12.1988577   3.9405172 \n         79          80          81          82          83          84 \n  2.1227701 -13.6997592   3.5268614 -10.4779208   8.7219714 -38.9087405 \n         85          86          87          88          89          90 \n -7.4206961  20.7486998 -20.5142136  13.4905686  -2.6029490  -7.8152229 \n         91          92          93          94          95          96 \n  3.0340348  15.7978497  10.7978497  27.7171892   2.0721306  -1.7360520 \n         97 \n 12.5840862"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPsy 8264",
    "section": "",
    "text": "Welcome to EPsy 8264: Advanced Multiple Regression Analysis. This is an advanced seminar for doctoral students in education covering a diverse set of regression methodologies. We will begin with a brief review of the General Linear Model and establishment of a mathematical foundation for the estimation of regression coefficients and standard errors in these models through the use of matrix algebra. The course will also cover more advanced modeling techniques, such as regression diagnostics, WLS and sandwich estimation, PCA, shrinkage methods, model selection and local models.\n\n\n\n\n\n\nTuesday/Thursday (9:45–11:00): Appleby Hall 204\n\n\n\n\n\n\nThe course syllabus is available here.\nMessage from Snoop Dogg about the syllabus\n\n\n\n\n\nThe course textbook is available via the University of Minnesota library.\n\nFox, J. (2013). A mathematical primer for social statistics. Sage.\n\n\n\n\n\n\n\nStatistical Modeling and Computation for Educational Scientists\n\nLearn about simple and multiple regression\nLearn about Ordinary Least squares (OLS)\nLearn about correlation/standardized regression\nLearn about regrression assumptions\nLearn about dummy coding for dichotomous/polychotomous predictors\nLearn about interaction effects\n\nComputational Toolkit for Educational Scientists\n\nLearn basics of R\nLearn {dplyr} for wrangling data\nLearn {ggplot2} for visualizing data\n\nEPsy 8252 Materials\n\nLearn about Maximum Likelihood\nLearn about information criteria\nLearn about polynomial effects\nLearn about log-transformations\nLearn about logistic models\nLearn about linear mixed-effects models"
  },
  {
    "objectID": "notes/03-ols-regression.html",
    "href": "notes/03-ols-regression.html",
    "title": "📝 OLS Regression Using Matrices and Its Properties",
    "section": "",
    "text": "Summation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this.\n\nThe handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest."
  },
  {
    "objectID": "notes/03-regression-diagnostics-2023.html",
    "href": "notes/03-regression-diagnostics-2023.html",
    "title": "📝 Regression Diagnostics",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to empirical diagnostics to detect extreme observations. We will use the contracption.csv data to evaluate the effect of fermale education level on contraception rates.\n\n[CSV]\n[Codebook]\n\nA script file for the analyses in these notes is also available:\n\n[R Script File]\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Import data\ncontraception = read_csv(file = \"~/Documents/github/epsy-8264/data/contraception.csv\")\n\n# View data\ncontraception\n\n\n\n  \n\n\n\n\n\nEffect of Female Education Level\nTo evaluate the effect of female education level on contraception rates, we need to fit a model that includes that effect. Since we know that a country’s wealth also has an impact on contraception rates, we also want to include that effect, which is measured by GNI, in the model.\n\n# Create dummy variable for GNI indicator and single letter variable\ncontraception = contraception %>%\n  mutate(\n    high_gni = if_else(gni == \"High\", 1, 0),\n    gni2 = str_sub(contraception$gni, start = 1L, end = 1L)\n  )\n\nWe will then examine a scatterplot of the data to determine whether we should fit a main effects or interaction model.\n\nggplot(data = contraception, aes(x = educ_female, y = contraceptive, color = gni2)) +\n  geom_text(aes(label = gni2)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  xlab(\"Female education level\") +\n  ylab(\"Contraceptive rate\") +\n  ggsci::scale_color_d3() +\n  guides(color = FALSE)\n\n\n\n\n\n\n\n\nThis plot suggests that the effect of female education level may differ for low and high wealth countries. This suggests that we may want to fit an intewraction model.\n\n# Fit interaction model\nlm.1 = lm(contraceptive ~ 1 + educ_female + high_gni + educ_female:high_gni, data = contraception)\n\n# Model-level information\nglance(lm.1)\n\n\n\n  \n\n\n# Coefficient-level information\ntidy(lm.1, conf.int = 0.95)\n\n\n\n  \n\n\n\nThe model explains 49.7% of the variation in contraception rates, \\(F(3, 97) = 30.6\\), \\(p<.001\\). Examining the coefficient-level output, the interaction term is not statistically significant, \\(p=0.341\\), indicating that there may not be an interaction between female education level and wealth on contraception.\n\n# Augment model\nout_1 = augment(lm.1)\n\n# View augmented data\nout_1\n\n\n\n  \n\n\n# Residual Plots\np1 = ggplot(data = out_1, aes(x = .resid)) +\n  educate::stat_density_confidence(model = \"normal\") +\n  geom_density() +\n  theme_bw() +\n  xlab(\"Residuals\")\n\np2 = ggplot(data = out_1, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0) +\n  theme_bw() +\n  xlab(\"Fitted values\") +\n  ylab(\"Residuals\")\n\n# Layout\np1 | p2\n\n\n\n\n\n\n\n\nThe model’s residuals do not seem to be consistent with the assumption of normality. Moreover, the assumption of homoskedasticity also seems to be violated, with the plot of the residuals versus the fitted values indicating increased variation in the residuals at higher fitted values. One question is whether the assumption violation is due to one or more extreme observations. For example, the three observations (54.1, -35.1), (70.4, -41.4) and (72.6, -32.6) all have extreme negative residuals. There are also a couple of observations that may have extreme positive residuals.\n\n\n\nIdentifying Extreme Observations\nIn a regression analysis, there are several ways in which an observation may be extreme. The figure below shows three different ways in which an observation may be extreme.\n\n\n\n\n\nThree different ways in which an observation may be extreme. The greenish observation is the extreme observation. The regression line is also displayed for the observations.\n\n\n\n\nIn Panel (a), the extreme observation has a large residual, but it’s x-value is similar to the x-values of the other observations. In Panel (b), the extreme observation has a large x-value relative to the other observations’ x-values, but it does not have a large residual. In Panel (c), the extreme observation has both a large x-value relative to the other observations’ x-values, and a large residual.\nWith extreme observations, we worry about whether the regression cioefficients, and thus the line, will be impacted. You can see how the regression line changes for each of these three types of outliers in the plot below.\n\n\n\n\n\nThree different ways in which an observation may be extreme. The greenish observation is the extreme observation. The regression line is also displayed for the observations."
  },
  {
    "objectID": "notes/04-simulating-from-the-regression-model.html",
    "href": "notes/04-simulating-from-the-regression-model.html",
    "title": "📝 Simulating from the Regression Model",
    "section": "",
    "text": "Generating Data\n\nGenerating Random Data from a Regression Model is a script file that provides syntax for generating data from a given population regression model.\n\n\n\n\nSimulation 1: Simulating from a Regression Model\n\nSimulating from a Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates from a regression model.\nVizualization of Simulating from a Regression Model is a handout visualizing the simulation process for generating data from a given population regression model.\n\n\n\n\nSimulation 2: Simulating from a Null Regression Model\n\nSimulating from a Null Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates assuming certain parameters in the regression model are zero.\nVizualization of Simulating from a Null Regression Model is a handout visualizing the simulation process for generating data to produce distributions of estimates assuming certain parameters in the regression model are zero.\n\n\n\n\nResources\nHere are several resources to help your understanding of simulation. The chapters from Monte Carlo Simulation and Resampling Methods for Social Science should be accessible via the links after logging in with your x500 and password.\n\nProbability: Common probability distributions and how to compute with them.\nRandom Number Generation: Learn how to draw random numbers from different distributions. Also information about repeating processes in R, including writing your own functions, using for loops, and using if-else functions."
  },
  {
    "objectID": "notes/05-regression-diagnostics.html",
    "href": "notes/05-regression-diagnostics.html",
    "title": "📝 Regression Diagnostics",
    "section": "",
    "text": "Slides is a set of slides we will cover in class.\nScript File is a script file that provides syntax for generating data from a given population regression model."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "",
    "text": "In this set of notes, we will use data from Statistics Canada’s Survey of Labour and Income Dynamics (SLID) to explain variation in the hourly wage rate of employed citizens in Ontario.\nA script file for the analyses in these notes is also available:"
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#violating-homoskedasticity",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Violating Homoskedasticity",
    "text": "Violating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\n\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\n\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes)."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#box-cox-transformation",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Box-Cox Transformation",
    "text": "Box-Cox Transformation\nIs there a power transformation that would better “fix” the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis transformation is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\).\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\nslid = slid %>%\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n\nCode\n# Examine residual plots\nresidual_plots(lm_bc)\n\n\n\n\n\nResidual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficient-level output:\n\ntidy(lm_bc, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y, controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?"
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#profile-plot-for-different-transformations",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Profile Plot for Different Transformations",
    "text": "Profile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\nPlot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n# Zomm in on confidence limits\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\nPlot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#assume-error-variances-are-known",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "Assume Error Variances are Known",
    "text": "Assume Error Variances are Known\nLet’s assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance–covariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\ndata.frame(\n  class_act = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2),\n  teacher_act = c(21, 20 , 19, 18, 17, 16),\n  class_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\n) %>%\n  kable(\n    col.names = c(\"Class Average ACT\", \"Teacher ACT\", \"Class SD\"),\n    align = \"c\"\n  ) %>%\n  kable_classic()\n\n\n\n \n  \n    Class Average ACT \n    Teacher ACT \n    Class SD \n  \n \n\n  \n    17.3 \n    21 \n    5.99 \n  \n  \n    17.1 \n    20 \n    3.94 \n  \n  \n    16.4 \n    19 \n    1.90 \n  \n  \n    16.4 \n    18 \n    0.40 \n  \n  \n    16.1 \n    17 \n    5.65 \n  \n  \n    16.2 \n    16 \n    2.59 \n  \n\n\n\n\n\nSuppose we want to use the teacher’s ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1)\nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n[1] 0.98356794 0.05294073\n\n\nWe could also have used built-in R functions to obtain these values:\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n\n\n  \n\n\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n[1] 1.17680463 0.06527187\n\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\n \n\n\nOLS\nWLS\n\n  \n    Coefficient \n    B \n    SE \n    B \n    SE \n  \n \n\n  \n    Intercept \n    12.0905 \n    0.9836 \n    13.4155 \n    1.1768 \n  \n  \n    Effect of Teacher ACT Score \n    0.2429 \n    0.0529 \n    0.1658 \n    0.0653 \n  \n\n\n\n\n\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n\n\n  \n\n\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input."
  },
  {
    "objectID": "notes/06-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "href": "notes/06-tools-for-dealing-with-heteroskedasticity.html#what-if-error-variances-are-unknown",
    "title": "📝 Tools for Dealing with Heteroskedasticity",
    "section": "What if Error Variances are Unknown?",
    "text": "What if Error Variances are Unknown?\nThe previous example assumed that the variance–covariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\n\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\n\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) %>%\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\nBefore examining any output from this model, let’s examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n\nCode\n# Examine residual plots\nresidual_plots(lm_step_5)\n\n\n\n\n\nResidual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)"
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html",
    "href": "notes/07-diagnosing-collinearity.html",
    "title": "📝 Diagnosing Collinearity",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to empirical diagnostics to detect collinearity. We will use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\nA script file for the analyses in these notes is also available:"
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#effects-of-collinearity",
    "href": "notes/07-diagnosing-collinearity.html#effects-of-collinearity",
    "title": "📝 Diagnosing Collinearity",
    "section": "Effects of Collinearity",
    "text": "Effects of Collinearity\nIf the design matrix is not of full rank, and \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nwhere\n\n\\(R^2_j\\) is the squared multiple correlation for the regression of \\(X_j\\) on the the other predictors;\n\\(S^2_j\\) is the sample variance of predictor \\(X_j\\) defined by \\(S^2_j = \\dfrac{\\sum(X_{ij}-\\bar{X}_j)^2}{n-1}\\);\n\\(\\sigma^2_{\\epsilon}\\) is the variance of the residuals based on regressing \\(Y\\) on all the \\(X\\)’s\n\n\nRecall that the multiple correlation is the correlation between the outcome and the predicted values.\n\nThe first term in this product is referred to as the variance inflation factor (VIF). When one of the predictors is perfectly collinear with the others, the value of \\(R^2_j\\) is 1 and the VIF is infinity. Thus the sampling variance of \\(B_j=\\infty\\)."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#perfect-collinearity-in-practice-model-mis-specification",
    "href": "notes/07-diagnosing-collinearity.html#perfect-collinearity-in-practice-model-mis-specification",
    "title": "📝 Diagnosing Collinearity",
    "section": "Perfect Collinearity in Practice: Model Mis-specification",
    "text": "Perfect Collinearity in Practice: Model Mis-specification\nIn practice, it is unlikely that you will have exact (or perfect) collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.\n\n# Create design matrix\nX = data.frame(\n  b_0 = rep(1, 5),\n  employed = c(1, 1, 0, 0, 1),\n  not_employed = c(0, 0, 1, 1, 0)\n)\n\n# View design matrix\nX\n\n\n\n  \n\n\n\nThe columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,\n\\[\nb_0 = 1(\\mathrm{employed}) + 1(\\mathrm{not~employed})\n\\]\nChecking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, X is not full column rank; it is rank deficient.\n\nMatrix::rankMatrix(X)\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 1.110223e-15\n\n\nIncluding all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.\n\nIncluding the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.\n\n\n# Create vector of outcomes\nY = c(15, 15, 10, 15, 30)\n\n# Create data frame of Y and X\nmy_data = cbind(Y, X)\nmy_data\n\n\n\n  \n\n\n# Coefficients (including all three terms)\ncoef(lm(Y ~ 1 + employed + not_employed, data = my_data))\n\n (Intercept)     employed not_employed \n        12.5          7.5           NA \n\n# Coefficients (omitting intercept)\ncoef(lm(Y ~ -1 + employed + not_employed, data = my_data))\n\n    employed not_employed \n        20.0         12.5 \n\n\nIf you overparameterize a model with lm(), one or more of the coefficients will not be estimated (the last parameters entered in the model).\n\nConstraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#non-exact-collinearity",
    "href": "notes/07-diagnosing-collinearity.html#non-exact-collinearity",
    "title": "📝 Diagnosing Collinearity",
    "section": "Non-Exact Collinearity",
    "text": "Non-Exact Collinearity\nIt is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients’ sampling variances. Again, we look toward how the sampling variances for the coefficent’s are computed:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nWhen the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between \\(X_j\\) and the other \\(X\\)s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model’s residual variance, sample size, and the predictor’s variance—the factors we typically think of affecting the sampling variance of a coefficient.\nIn cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between \\(X_j\\) and the other \\(X\\)s is larger than 0. (Perfect collinearity results in \\(R^2_j=1\\).) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).\nHow much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations (\\(R_j\\)) between \\(X_j\\) and the predicted values from using the other \\(X\\)’s to predict \\(X_j\\).\n\n\n\n\nImpact of various Rj values on the VIF and size of the CI for Bj.\n \n  \n    Rj \n    VIF \n    CI Factor \n  \n \n\n  \n    0.0 \n    1.00 \n    1.00 \n  \n  \n    0.1 \n    1.01 \n    1.01 \n  \n  \n    0.2 \n    1.04 \n    1.02 \n  \n  \n    0.3 \n    1.10 \n    1.05 \n  \n  \n    0.4 \n    1.19 \n    1.09 \n  \n  \n    0.5 \n    1.33 \n    1.15 \n  \n  \n    0.6 \n    1.56 \n    1.25 \n  \n  \n    0.7 \n    1.96 \n    1.40 \n  \n  \n    0.8 \n    2.78 \n    1.67 \n  \n  \n    0.9 \n    5.26 \n    2.29 \n  \n  \n    1.0 \n    Inf \n    Inf \n  \n\n\n\n\n\nFor example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of \\(B_j\\).\nTo sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#collinearity-diagnostics",
    "href": "notes/07-diagnosing-collinearity.html#collinearity-diagnostics",
    "title": "📝 Diagnosing Collinearity",
    "section": "Collinearity Diagnostics",
    "text": "Collinearity Diagnostics\nWe can also empirically diagnose problematic collinearity in the data (D. A. Belsley, 1991; D. Belsley et al., 1980). Before we do, however, it is important that the functional form of the model has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated after the model has been satisfactorily specified.\nBelow we will explore some of the diagnostic tools available to an applied researcher."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#high-correlations-among-predictors",
    "href": "notes/07-diagnosing-collinearity.html#high-correlations-among-predictors",
    "title": "📝 Diagnosing Collinearity",
    "section": "High Correlations among Predictors",
    "text": "High Correlations among Predictors\nCollinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.\n\neeo %>%\n  select(faculty, peer, school) %>%\n  correlate()\n\n\n\n  \n\n\n\nIn this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step)."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#regress-each-predictor-on-the-other-predictors",
    "href": "notes/07-diagnosing-collinearity.html#regress-each-predictor-on-the-other-predictors",
    "title": "📝 Diagnosing Collinearity",
    "section": "Regress each Predictor on the Other Predictors",
    "text": "Regress each Predictor on the Other Predictors\nSince collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the \\(R^2\\) value. If all the \\(R^2\\) values are close to zero there is no collinearity problems. If one or more of the \\(R^2\\) values are close to 1, there is a collinearity problem.\n\n# Use faculty as outcome; obtain R2\nsummary(lm(faculty ~ 1 + peer + school, data = eeo))$r.squared\n\n[1] 0.9733906\n\n# Use faculty as outcome; obtain R2\nsummary(lm(peer ~ 1 + faculty + school, data = eeo))$r.squared\n\n[1] 0.9669002\n\n# Use faculty as outcome; obtain R2\nsummary(lm(school ~ 1 + faculty + peer, data = eeo))$r.squared\n\n[1] 0.9879743\n\n\nAll three \\(R^2\\) values are quite high, which is indicative of collinearity.\nOne shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the \\(R^2\\) values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.\nWe will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#variance-inflation-factor-vif",
    "href": "notes/07-diagnosing-collinearity.html#variance-inflation-factor-vif",
    "title": "📝 Diagnosing Collinearity",
    "section": "Variance Inflation Factor (VIF)",
    "text": "Variance Inflation Factor (VIF)\nPerhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:\n\\[\n\\mathrm{VIF} = \\frac{1}{1 - R^2_j}\n\\]\nThe VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the vif() function from the car package to compute the variance inflation factors for each coefficient.\n\n# VIF\nvif(lm.1)\n\n faculty     peer   school \n37.58064 30.21166 83.15544 \n\n# Square root of VIF\nsqrt(vif(lm.1))\n\n faculty     peer   school \n6.130305 5.496513 9.118960 \n\n\nThe variances (and hence, the standard errors) for all three coefficients are inflated because of collinearity. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.\nRemember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite \\(\\mathrm{VIF}>10\\) as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite \\(\\mathrm{VIF}>4\\) as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.\nThere are several situations in which high VIF values are expected and not problematic:\n\nThe variables with high VIFs are control variables, and the variables of interest do not have high VIFs. Since we would not be interested in inference around the control variables, high VIF values on those variables would not\nThe high VIFs are caused by the inclusion of powers or products of other variables. The p-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the p-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.\nThe variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. This is especially true when the reference category used has a small proportion of cases. In this case, p-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#eigenvalues-of-the-correlation-matrix",
    "href": "notes/07-diagnosing-collinearity.html#eigenvalues-of-the-correlation-matrix",
    "title": "📝 Diagnosing Collinearity",
    "section": "Eigenvalues of the Correlation Matrix",
    "text": "Eigenvalues of the Correlation Matrix\nA second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square (\\(k \\times k\\)) matrix has a set of k scalars, called eigenvalues (denoted \\(\\lambda\\)) associated with it. These eigenvalues can be arranged in descending order such that,\n\\[\n\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_k\n\\]\nBecause any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix.\nAs a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:\n\\[\n\\sum_{i=1}^k \\frac{1}{\\lambda_i}\n\\]\n\nIf the predictors are orthogonal to one another (independent) then \\(\\lambda_i = 1\\) and the sum of the reciprocal values will be equal to the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = k\\).\nIf the predictors are collinear with one another (dependent) then \\(\\lambda_i = 0\\) and the sum of the reciprocal values will be equal to infinity, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = \\infty\\).\nWhen there is nonperfect collinearity then \\(0 < \\lambda_i < 1\\), and the sum of the reciprocal values will be greater than the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} > k\\).\n\n\nIn an orthogonal matrix, the eigenvalues are all \\(\\pm1\\), but since the correlation matrix is positive semidefinite, the eigenvalues are all \\(+1\\).\n\nLarger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.\n\\[\n\\mathrm{IF} \\quad \\sum_{i=1}^k \\frac{1}{\\lambda_i} > 5k \\quad \\mathrm{THEN} \\quad \\mathrm{collnearity~is~a~problem}\n\\]\n\n\nIn practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.\n\n\n\nUsing R to Compute the Eigenvalues of the Correlation Matrix\nBecause collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the eigen() function to compute the eigenvalues of a square matrix.\nIn previous classes, I have been using the correlate() function from the {corrr} package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the cor() function, which produces a matrix object, to produce the correlation matrix.\n\n# Correlation matrix of predictors\nr_xx = cor(eeo[c(\"faculty\", \"peer\", \"school\")])\nr_xx\n\n          faculty      peer    school\nfaculty 1.0000000 0.9600806 0.9856837\npeer    0.9600806 1.0000000 0.9821601\nschool  0.9856837 0.9821601 1.0000000\n\n\nOnce we have the correlation matrix, we can use the eigen() function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.\n\n# Compute eigenvalues and eigenvectors\neigen(r_xx)\n\neigen() decomposition\n$values\n[1] 2.951993158 0.040047507 0.007959335\n\n$vectors\n           [,1]        [,2]       [,3]\n[1,] -0.5761385  0.67939712 -0.4544052\n[2,] -0.5754361 -0.73197527 -0.3648089\n[3,] -0.5804634  0.05130072  0.8126687\n\n# Sum of reciprocal of eigenvalues\nsum(1 / eigen(r_xx)$values)\n\n[1] 150.9477\n\n\nWe compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; \\(5 \\times 3 =15\\). Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model."
  },
  {
    "objectID": "notes/07-diagnosing-collinearity.html#condition-indices",
    "href": "notes/07-diagnosing-collinearity.html#condition-indices",
    "title": "📝 Diagnosing Collinearity",
    "section": "Condition Indices",
    "text": "Condition Indices\nA condition number for a matrix and computational task quantifies how sensitive the result is to perturbations in the input data and to roundoff errors made during the solution process. If minor changes to the matrix elements result in large differences in the computation (say of the inverse), we say that the matrix is “ill-conditioned”. It is important to note that a condition number applies not only to a particular matrix, but also to the particular computation being carried out. That is, a matrix can be ill-conditioned for inversion while the eigenvalue problem is well-conditioned.\nA third common diagnostic measure of collinearity is to compute the condition number of the correlation matrix of the model predictors. This tells us whether small changes in the data will lead to large changes in regression coefficient estimates. To compute the condition number, we need to compute the condition index for each of the eigenvalues of the correlation matrix based on the model predictors. Each eigenvalue has an associated condition index, and the jth eigenvalue’s condition index is denoted \\(\\kappa_j\\), where,\n\\[\n\\kappa_j = \\sqrt{\\frac{\\lambda_{\\mathrm{Max}}}{\\lambda_j}}\n\\]\nand \\(\\lambda_{\\mathrm{Max}}\\) is the largest eigenvalue. The largest eigenvalue will have a condition index of,\n\\[\n\\begin{split}\n\\kappa_{\\lambda_\\mathrm{Max}} &= \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Max}}} \\\\[1ex]\n&= 1\n\\end{split}\n\\]\nThe smallest eigenvalue will have a condition index of\n\\[\n\\kappa_{\\lambda_\\mathrm{Min}} = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}}\n\\]\nThis value will be larger than 1 since \\(\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}\\) will be greater than 1. In general, the largest eigenvalue will have a condition index of 1 and the other condition indices for every other eigenvalue will be larger than one.\nThe condition number of the correlation matrix is equivalent to the condition index for the smallest eigenvalue, that is,\n\\[\n\\kappa = \\sqrt{\\frac{\\lambda_\\mathrm{Max}}{\\lambda_\\mathrm{Min}}}\n\\]\nIf the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.\nFrom empirical work, a condition number between 10 and 30 indicates the presence of multicollinearity. When the condition number is larger than 30, the multicollinearity is regarded as strong and corrective action will almost surely need to be taken. Below we compute the condition indices and the condition number for our empirical example.\n\n# Sort eigenvalues from largest to smallest\nlambda = sort(eigen(r_xx)$values, decreasing = TRUE)\n\n# View eigenvalues\nlambda\n\n[1] 2.951993158 0.040047507 0.007959335\n\n# Compute condition indices\nsqrt(max(lambda) / lambda)\n\n[1]  1.000000  8.585586 19.258359\n\n# Compute condition number directly\nsqrt(max(lambda) / min(lambda))\n\n[1] 19.25836\n\n\nThe condition number of the correlation matrix, \\(\\kappa = 19.26\\), suggests there is collinearity among the predictors."
  },
  {
    "objectID": "notes/08-pca-via-spectral-decomposition.html",
    "href": "notes/08-pca-via-spectral-decomposition.html",
    "title": "📝 Principal Components Analysis via Spectral Decomposition",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to principal components analysis via spectral decomposition. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\nA script file for the analyses in these notes is also available:\nThe problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. One method to deal with collinearity among a set of predictors is to combine the predictors into a smaller subset of orthogonal measures (called principal components) that can be used instead of the original predictors. This subset of measures will not have the collinearity problems (they are orthogonal to one another), but constitute a slightly smaller amount of “variance accounted for” than the original set of predictors."
  },
  {
    "objectID": "notes/08-pca-via-spectral-decomposition.html#matrix-algebra-to-carry-out-the-pca-using-spectral-decomposition",
    "href": "notes/08-pca-via-spectral-decomposition.html#matrix-algebra-to-carry-out-the-pca-using-spectral-decomposition",
    "title": "📝 Principal Components Analysis via Spectral Decomposition",
    "section": "Matrix Algebra to Carry Out the PCA using Spectral Decomposition",
    "text": "Matrix Algebra to Carry Out the PCA using Spectral Decomposition\nTo carry out the spectral decomposition in R, we need to create the matrix of the predictors (\\(\\mathbf{X}_p\\)), compute the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, and then use the eigen() function to carry out the spectral decomposition.\n\n# Create predictor matrix\nX_p = eeo %>%\n  select(peer, faculty) %>%\n  data.matrix()\n\n# Spectral decomposition\nspec_decomp = eigen(t(X_p) %*% X_p)\nspec_decomp\n\neigen() decomposition\n$values\n[1] 137.561000   2.724415\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6469680 -0.7625172\n[2,] 0.7625172  0.6469680\n\n\nThe matrix of eigenvectors, in the $vectors component, compose the P matrix and make up the set of basis vectors for the rotated predictor space. The elements in the $values component are the diagonal elements in D and are the eigenvalues. The decomposition is:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_p\\mathbf{X}_p &= \\mathbf{PDP}^\\intercal \\\\[1em]\n\\begin{bmatrix}59.16 & 66.52 \\\\ 66.62 & 81.12\\end{bmatrix} &= \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix} \\begin{bmatrix}137.561 & 0  \\\\ 0 & 2.724 \\end{bmatrix} \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix}^\\intercal\n\\end{split}\n\\]\nThe span of the basis vectors define the principal components, with the first eigenvector defining the first principal component and the second eigenvector defining the second principal component. (Note: There number of principal components will always be the same as the number of predictors in the \\(\\mathbf{X}_p\\) matrix.)\nWe can post-multiply the matrix of the original predictor values (in \\(\\mathbf{X}_p\\)) by this matrix of basis vectors to obtain the predictor values in the rotated space.\n\n# Matrix of basis vectors for rotated predictor space\nrot_basis = spec_decomp$vectors\n\n# Compute rotated values under the new basis\nrot_pred = X_p %*% rot_basis\nhead(rot_pred)\n\n            [,1]        [,2]\n[1,]  0.48641930  0.36669037\n[2,]  0.91525519  0.14806327\n[3,] -1.03087107 -0.06220262\n[4,] -1.74270855  0.11707722\n[5,]  0.01287131  0.25376126\n[6,]  0.23695822  0.03365744\n\n\nFor example, the first observation, has a value of 0.486 on the first principal component and a value of 0.367 on the second principal component. Similarly, the second observation has a value of 0.915 on the first principal component and a value of 0.148 on the second principal component. Each observation has a set of values on the principal components.\nThe eigenvalues are related to the variances of the principal components. Because we decomposed the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, the variance on each prinicpal component can be computed as:\n\\[\n\\mathrm{Var}(\\mathrm{PC}_i) = \\frac{\\lambda_i}{n-1}\n\\]\nIn our example, the variances can be computed as:\n\n# Compute variances of PCs\nvar_pc = spec_decomp$values / (70 - 1)\nvar_pc\n\n[1] 1.99363769 0.03948427\n\n\nThe first principal component has the largest variance, which will always be the case. Remember, the principal components are selected so the first component maximizes the variation in the predictor space, the second component will maximize the remaining variance (and be orthogonal to the first), etc.\nWe can use these variances to determine the proportion of variation in the predictor space that each principal component accounts for. This is often more useful to the applied data analyst than the actual variance measure itself. Since the principal components are orthogonal, we can sum the variances to obtain a total measure of variation in the original set of predictors accounted for by the principal components. Below, we compute the proportion of variance in the predictor space that each principal component in our example accounts for:\n\n# Compute proportion of variation\nvar_pc / sum(var_pc)\n\n[1] 0.98057949 0.01942051\n\n\nHere the first principal component accounts for 98.1% of the variation in the predictor space, and the second principal component accounts for the remaining 1.9% of the variation."
  },
  {
    "objectID": "notes/08-pca-via-spectral-decomposition.html#using-princomp-to-obtain-the-principal-components",
    "href": "notes/08-pca-via-spectral-decomposition.html#using-princomp-to-obtain-the-principal-components",
    "title": "📝 Principal Components Analysis via Spectral Decomposition",
    "section": "Using princomp() to Obtain the Principal Components",
    "text": "Using princomp() to Obtain the Principal Components\nWe can also use the R function princomp() to obtain the principal components based on the spectral decomposition. We provide this function with a data frame of the predictors.\n\n# Select predictors\neeo_pred = eeo %>%\n  select(faculty, peer)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE)\n\nImportance of components:\n                          Comp.1     Comp.2\nStandard deviation     1.4002088 0.19725327\nProportion of Variance 0.9805406 0.01945935\nCumulative Proportion  0.9805406 1.00000000\n\nLoadings:\n        Comp.1 Comp.2\nfaculty  0.763  0.647\npeer     0.647 -0.763\n\n\nThe values of the principal components, the rotated set of basis vectors, are given in the loadings output. Note that the signs of the principal components are arbitrary. For example, the vector associated with the first principal component could also have been \\((-0.763, -0.647)\\) and that for the second principal component could have been \\((-0.647, 0.763)\\). The variances of each component can be computed by squaring the appropriate standard deviations in the output.\n\n# Compute variance of PC1\n1.4002088 ^ 2\n\n[1] 1.960585\n\n# Compute variance of PC2\n0.19725327 ^ 2\n\n[1] 0.03890885\n\n\nWe can use the variance measures to obtain a total measure of variation in the original set of predictors accounted for by each of the principal components.\n\n# Compute total variation accounted for\ntotal_var = 1.4002088 ^ 2 + 0.19725327 ^ 2\ntotal_var\n\n[1] 1.999494\n\n# Compute variation accounted for by PC1\n(1.4002088 ^ 2) / total_var\n\n[1] 0.9805406\n\n# Compute variation accounted for by PC2\n(0.19725327 ^ 2) / total_var\n\n[1] 0.01945935\n\n\nThis suggests that the first principal component accounts for 98% of the variance in the original set of predictors and that the second principal component accounts for 2% of the variance. (Same as we computed in the matrix algebra.) Note that these values are also given in the summary() output.\nWe can also obtain the principal component scores (the values under the rotation) for each observation by accessing the scores element of the princomp object. (Below we only show the first six scores.)\n\n# Get PC scores\npc_scores = my_pca$scores\n\n# View PC scores\nhead(pc_scores)\n\n          Comp.1      Comp.2\n[1,]  0.41884376  0.37000669\n[2,]  0.84765375  0.15132881\n[3,] -1.09849740 -0.05870659\n[4,] -1.81031365  0.12065755\n[5,] -0.05471761  0.25713367\n[6,]  0.16934323  0.03700331\n\n\nThese are slightly different than the scores we obtained by multiplying the original predictor values by the new basis matrix. For example, the PC scores for the first observation were \\(-0.486\\) and \\(0.367\\). The princomp() function mean centers each variable prior to multiplying by the basis matrix.\n\n# Mimic scores from princomp()\nold = t(c(0.608 - mean(eeo$faculty), 0.0351 - mean(eeo$peer)))\n\n# Compute PC scores\nold %*% basis\n\n          [,1]      [,2]\n[1,] 0.4186997 0.3699581\n\n\n\nBecause we want the principal components to be solely functions of the predictors, we mean center them. Otherwise we would have to include a column of ones (intercept) in the \\(\\mathbf{X}_p\\) matrix. Mean centering the predictors makes each predictor orthogonal to the intercept, in which case it can be ignored. (This is similar to how mean centering predictors removes the intercept from the fitted equation.)\n\nSince we only have two principal components, we can visualize the scores using a scatterplot.\n\n# Create data frame of scores\npc_scores = pc_scores %>%\n  data.frame()\n\n# Plot the scores\nggplot(data = pc_scores, aes(x = Comp.1, y = Comp.2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"lightgrey\") +\n  geom_vline(xintercept = 0, color = \"lightgrey\") +\n  scale_x_continuous(name = \"Principal Component 1\", limits = c(-4, 4)) +\n  scale_y_continuous(name = \"Principal Component 2\", limits = c(-4, 4)) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nRotated predictor space using the principal components as the new basis.\n\n\n\n\nConceptually this visualization shows the rotated predictor space after re-orienting the rotated coordinate system. From this visualization, it is also clear that there is much more variation in the values of the first principal component than in the second principal component."
  },
  {
    "objectID": "notes/09-pca-via-svd.html",
    "href": "notes/09-pca-via-svd.html",
    "title": "📝 Principal Components Analysis via Singular Value Decomposition",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to principal components analysis via singular value decomposition. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\nA script file for the analyses in these notes is also available:"
  },
  {
    "objectID": "notes/09-pca-via-svd.html#pca-using-svd-matrix-algebra",
    "href": "notes/09-pca-via-svd.html#pca-using-svd-matrix-algebra",
    "title": "📝 Principal Components Analysis via Singular Value Decomposition",
    "section": "PCA using SVD: Matrix Algebra",
    "text": "PCA using SVD: Matrix Algebra\nTo carry out a principal components analysis, we need to use SVD to decompose the matrix \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\), where \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) is a matrix of the predictors being used in the PCA. Below, we create the \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix we use the svd() function to decompose the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix using singular value decomposition.\n\n# Create matrix of predictors\nX_p = as.matrix(eeo[ , c(\"faculty\", \"peer\", \"school\")])\n\n# SVD decomposition\nsv_decomp = svd(t(X_p) %*% X_p)\n\n# View results\nsv_decomp\n\n$d\n[1] 209.3295610   2.7303093   0.5833274\n\n$u\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n$v\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[1em]\n\\begin{bmatrix}81.123 & 66.518 & 75.512 \\\\ 66.518 & 59.163 & 64.251 \\\\ 75.512 & 64.251 & 72.358\\end{bmatrix} &= \\begin{bmatrix}0.617 & 0.670 & -0.412 \\\\ -0.524 & -0.741 & -0.419 \\\\ -0.586 & -0.042 & 0.809\\end{bmatrix} \\begin{bmatrix}209.330 & 0 & 0 \\\\ 0 & 2.730 & 0 \\\\ 0 & 0 & 0.583 \\end{bmatrix} \\begin{bmatrix}-0.617 & -0.524 & -0.586 \\\\ 0.670 & -0.741 & -0.042 \\\\ -0.412 & -0.419 &  0.809 \\end{bmatrix}\n\\end{split}\n\\]"
  },
  {
    "objectID": "notes/09-pca-via-svd.html#understanding-what-the-matrix-algebra-is-doing",
    "href": "notes/09-pca-via-svd.html#understanding-what-the-matrix-algebra-is-doing",
    "title": "📝 Principal Components Analysis via Singular Value Decomposition",
    "section": "Understanding What the Matrix Algebra is Doing",
    "text": "Understanding What the Matrix Algebra is Doing\nMathematically, since any matrix can be decomposed using SVD, we can also decompose \\(\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\\). Then we can write the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix, \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), as:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})^{\\intercal} (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})\n\\]\nRe-expressing this we get:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[0.5em]\n\\end{split}\n\\]\nSince D is a diagonal matrix, \\(\\mathbf{D}^{\\intercal}\\mathbf{D} = \\mathbf{D}^2\\), so reducing this expression gives:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^{\\intercal}\n\\]\nThe matrices V and \\(\\mathbf{V}^{\\intercal}\\) are both orthogonal basis matrices that ultimately act to change the coordinate system by rotating the original basis vectors used in the predictor space. The \\(\\mathbf{D}^2\\) matrix is diagonalizing the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix which amounts to finding the the major axes in the data ellipse along which our data varies."
  },
  {
    "objectID": "notes/09-pca-via-svd.html#scaling-the-predictors",
    "href": "notes/09-pca-via-svd.html#scaling-the-predictors",
    "title": "📝 Principal Components Analysis via Singular Value Decomposition",
    "section": "Scaling the Predictors",
    "text": "Scaling the Predictors\nIn practice, it is important to scale all the predictors used in the PCA. This is especially true when the variables are measured in different metrics or have varying degrees of magnitude. In these cases, not scaling the predictors will often result in results in which variables with large magnitudes of scale dominate the PCA. It also is helpful when the predictors are measured using qualitatively different scales (e.g., one is measured in dollars and another in years of education).\n\nRecall, the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix for a set of standardized predictors his the correlation matrix of the predictors. Thus, the SVD is actually being carried out on the correlation matrix rather than the raw \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix.\n\nWhile it isn’t necessary to also center the predictors, it is common to simply standardize the set of predictors being used in the PCA (i.e., convert them to z-scores); thus both centering and scaling them. To do this, we will pipe the selected predictors into the scale() function prior to piping into the prcomp() function.\n\n# Fit the PCA using SVD decomposition on standardized predictors\nsvd_pca_z = eeo %>%\n  select(faculty, peer, school) %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  prcomp()\n\n# View standard deviations and rotation matrix (eigenvector matrix)\nsvd_pca_z\n\nStandard deviations (1, .., p=3):\n[1] 1.71813654 0.20011873 0.08921511\n\nRotation (n x k) = (3 x 3):\n               PC1         PC2        PC3\nfaculty -0.5761385  0.67939712 -0.4544052\npeer    -0.5754361 -0.73197527 -0.3648089\nschool  -0.5804634  0.05130072  0.8126687\n\n# tidy version of the rotation matrix (good for graphing)\nsvd_pca_z %>%\n  tidy(matrix = \"rotation\")\n\n\n\n  \n\n\n# View sds, variance accounted for\nsvd_pca_z %>%\n  tidy(matrix = \"eigenvalues\")\n\n\n\n  \n\n\n# Obtain PC scores\npc_scores = augment(svd_pca_z)\npc_scores\n\n\n\n  \n\n\n\nHere the results from using the standardized predictors are not that different from the previous results since the variables were already reported as z-scores to begin with. The variance accounted for by the principal components is comparable (within rounding); the standardization of the predictors does not change this. The actual principal components in the V (rotation) matrix are different because of the centering and scaling, but the interpretations are the same as when we used the decomposition based on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. Similarly, because of the centering and scaling, the PC scores are different."
  },
  {
    "objectID": "notes/09-pca-via-svd.html#behind-the-scenes",
    "href": "notes/09-pca-via-svd.html#behind-the-scenes",
    "title": "📝 Principal Components Analysis via Singular Value Decomposition",
    "section": "Behind the Scenes",
    "text": "Behind the Scenes\nBy standardizing the predictors, we are carrying out the SVD on the correlation matrix of the predictors rather than on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. To see this, recall that the correlation matrix of the predictors (\\(\\mathbf{R_X}\\)) is based on the standardized predictors, namely,\n\nThis implies that you can also carry out PCA on summaries of the predictors (rather than the raw data) by decomposing either the covariance matrix of the predictors or the correlation matrix of the predictors. This can be useful for example, when trying to reproduce the results of a PCA from a published paper, in which authors will often report summaries of the data used (e.g., correlation matrices) but not the raw data.\n\n\\[\n\\mathbf{R_X} = \\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big)\n\\]\nThe \\(1/(n-1)\\) component is a scalar and is pulled out so that the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\) matrix:\n\\[\n\\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big) = \\frac{1}{n-1}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\n\\]\nSimilarly, we can also decompose the covariance matrix of the predictors (\\(\\boldsymbol\\Sigma_{\\mathbf{X}}\\)), which is based on the centered predictors,\n\\[\n\\boldsymbol\\Sigma_{\\mathbf{X}} = \\frac{1}{n-1} \\big( \\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\big)\n\\]\nIn this case, the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\) matrix. To do this using prcomp() we would change the scale= argument to FALSE in the scale() function, while still leaving center=TRUE."
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html",
    "href": "notes/10-biased-estimation-ridge-regression.html",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "",
    "text": "In this set of notes, we will give a brief introduction to ridge regression. We will continue to use the equal-education-opportunity.csv data provided from Chatterjee & Hadi (2012) to evaluate the availability of equal educational opportunity in public education. The goal of the regression analysis is to examine whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\nA script file for the analyses in these notes is also available:"
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#matrix-formulation-of-ridge-regression",
    "href": "notes/10-biased-estimation-ridge-regression.html#matrix-formulation-of-ridge-regression",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "Matrix Formulation of Ridge Regression",
    "text": "Matrix Formulation of Ridge Regression\nRecall that the OLS estimates are given by:\n\\[\n\\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nUnder collinearity, the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix is ill-conditioned. (A matrix is said to be ill-conditioned if it has a high condition number, meaning that small changes in the data impact the regression results. This ill-conditioning results in inaccuracy when we compute the inverse of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix, which translates into bad estimates of the coefficients and standard errors.\nTo see this, consider our EEO example data. We first standardize the variables (so we can omit the ones column in the design matrix) using the scale() function. Note that the output of the scale() function is a matrix. Then, we will select the predictors using indexing and compute the condition number for the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix.\n\n# Standardize all variables in the eeo data frame\nz_eeo = eeo %>% \n  scale()\n\n# Create and view the design matrix\nX = z_eeo[ , c(\"faculty\", \"peer\", \"school\")]\nhead(X)\n\n        faculty        peer     school\n[1,]  0.5158617 -0.01213684  0.1310782\n[2,]  0.6871673  0.46812962  0.4901170\n[3,] -0.8084583 -0.71996624 -0.7994390\n[4,] -1.2024935 -1.36577136 -1.0479983\n[5,]  0.1150408 -0.25030749  0.1078451\n[6,]  0.1413252  0.08793894  0.2356566\n\n# Get eigenvalues\neig_val = eigen(t(X) %*% X)$values\n\n# Compute condition number\nsqrt(max(eig_val) / min(eig_val))\n\n[1] 19.25836\n\n\nWe can inflate the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) to better condition the matrix. This, hopefully, leads to more stability in the inverse matrix and produces better (albeit biased) coefficient estimates. To do this, we can add some constant amount (\\(\\lambda\\)) to each of the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), prior to finding the inverse. This can be expressed as:\n\\[\n\\widetilde{\\mathbf{b}} = (\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{Y}\n\\]\nwhere the tilde over b indicates that the coefficients are biased. To see how increasing the diagonal values leads to better conditioning, we will add some value (here \\(\\lambda=10\\), but it could be any value between 0 and positive infinity) to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix in our example.\n\nTechnically this equation is for standardized variables; it assumes that there is no ones column in the X matrix. This is because we only want to add the \\(\\lambda\\) value to the parts of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix associated with the predictors.\n\n\n# Add 50 to each of the diagonal elements of X^T(X)\ninflated = t(X) %*% X + 10*diag(3)\n\n# Get eigenvalues\neig_val_inflated = eigen(inflated)$values\n\n# Compute condition number\nsqrt(max(eig_val_inflated) / min(eig_val_inflated))\n\n[1] 4.500699\n\n\nThe condition number has decreased from 19.26 (problematic collinearity) to 4.5 (non-collinear). Adding 10 to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix resulted in a better conditioned matrix!"
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#using-an-built-in-r-function",
    "href": "notes/10-biased-estimation-ridge-regression.html#using-an-built-in-r-function",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "Using an Built-In R Function",
    "text": "Using an Built-In R Function\nWe can also use the lm.ridge() function from the {MASS} package to fit a ridge regression. This function uses a formula based on variables from a data frame in the same fashion as the lm() function. It also takes the argument lambda= which specifies the values of \\(\\lambda\\) to use in the penalty term. Because the data= argument has to be a data frame (or tibble), we convert the standardized data (which is a matrix) to a data frame using the data.frame() function.\n\n# Create data frame for use in lm.ridge()\nz_data = z_eeo %>%\n  data.frame()\n\n# Fit ridge regression (lambda = 0.1)\nridge_1 = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = 0.1)\n\n# View coefficients\ntidy(ridge_1)"
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#comparison-to-the-ols-coefficients",
    "href": "notes/10-biased-estimation-ridge-regression.html#comparison-to-the-ols-coefficients",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "Comparison to the OLS Coefficients",
    "text": "Comparison to the OLS Coefficients\nHow do these biased coefficients from the ridge regression compare to the unbiased coefficients from the OLS estimation? Below, we fit a standardized OLS model to the data, and compare the coefficients to those from the ridge regression with \\(\\lambda=0.1\\).\n\n# Fit standardized OLS model\nlm.1 = lm(achievement ~ faculty + peer + school - 1, data = z_data)\n\n# Obtain coefficients\ncoef(lm.1)\n\n   faculty       peer     school \n 0.5248647  0.9449056 -1.0272986 \n\n\nComparing these coefficients from the different models:\n\n\n\n\nComparison of the coefficients from the OLS ($\\lambda=0$) and the ridge regression using $\\lambda=0.1$ based on the standardized data.\n \n  \n    Predictor \n    $\\lambda=0$ \n    $\\lambda=0.1$ \n  \n \n\n  \n    Faculty \n    0.525 \n    0.436 \n  \n  \n    Peer \n    0.945 \n    0.856 \n  \n  \n    School \n    -1.027 \n    -0.851 \n  \n\n\n\n\n\nBased on this comparison, the ridge regression has “shrunk” the estimate of each coefficient toward zero. Remember, the larger the value of \\(\\lambda\\), the more the coefficient estimates will shrink toward 0. The table below shows the coefficient estimates for four different values of \\(\\lambda\\).\n\n\n\n\nComparison of the coefficients from the OLS ($\\lambda=0$) and three ridge regressions (using $\\lambda=0.1$, $\\lambda=1$, and $\\lambda=10$) based on the standardized data. The condition numbers for the $\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}$ matrix are also provided.\n \n  \n    Predictor \n    $\\lambda=0$ \n    $\\lambda=0.1$ \n    $\\lambda=1$ \n    $\\lambda=10$ \n  \n \n\n  \n    Faculty \n    0.525 \n    0.436 \n    0.180 \n    0.114 \n  \n  \n    Peer \n    0.945 \n    0.856 \n    0.537 \n    0.227 \n  \n  \n    School \n    -1.027 \n    -0.851 \n    -0.283 \n    0.073 \n  \n  \n    Condition Number \n    370.884 \n    313.908 \n    132.125 \n    20.256 \n  \n\n\n\n\n\nExamining these results we see that increasing the penalty (i.e., higher \\(\\lambda\\) values) better conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but a higher penalty also shrinks the estimates toward zero more (increased bias). This implies that we want a \\(\\lambda\\) that is large enough so that it conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but not too large because we want to introduce the least amount of bias as possible."
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#ridge-trace",
    "href": "notes/10-biased-estimation-ridge-regression.html#ridge-trace",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "Ridge Trace",
    "text": "Ridge Trace\nA ridge trace computes the ridge regression coefficients for many different values of \\(\\lambda\\). A plot of this trace, can be examined to select the \\(\\lambda\\) value. To do this, we pick the smallest value for \\(\\lambda\\) that produces stable regression coefficients. Here we examine the values of \\(\\lambda\\) where \\(\\lambda = \\{ 0,0.001,0.002,0.003,\\ldots,100\\}\\).\nTo create this plot, we fit a ridge regression that includes a sequence of values in the lambda= argument of the lm.ridge() function. Then we use the tidy() function to summarize the output from this model. This output includes the coefficient estimates for each of the \\(\\lambda\\) values in our sequence. We can then create a line plot of the coefficient values versus the \\(\\lambda\\) values for each predictor.\n\n# Fit ridge model across several lambda values\nridge_models = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, \n                        lambda = seq(from = 0, to = 100, by = 0.01))\n\n# Get tidy() output\nridge_trace = tidy(ridge_models)\nridge_trace\n\n\n\n\n\nRidge plot showing the size of the standardized regression coefficients for \\(\\lambda\\) values between 0 (OLS) and 100.\n\n# Ridge trace\nggplot(data = ridge_trace, aes(x = lambda, y = estimate)) +\n  geom_line(aes(group = term, color = term)) +\n  theme_bw() +\n  xlab(\"d value\") +\n  ylab(\"Coefficient estimate\") +\n  ggsci::scale_color_d3(name = \"Predictor\")\n\n\n\n\nRidge plot showing the size of the standardized regression coefficients for \\(\\lambda\\) values between 0 (OLS) and 100.\n\n\n\n\nWe want to find the \\(\\lambda\\) value where the lines begin to flatten out; where the coefficients are no longer changing value. This is difficult to ascertain, but somewhere around \\(\\lambda=50\\), there doesn’t seem to be a lot of change in the coefficients. This suggests that a \\(\\lambda\\) value around 50 would produce stable coefficient estimates."
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#aic-value",
    "href": "notes/10-biased-estimation-ridge-regression.html#aic-value",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "AIC Value",
    "text": "AIC Value\nIt turns out that not only is it difficult to make a subjective call about where the trace lines begin to flatten, but even when people do make this determination, they often select a \\(\\lambda\\) value that is too high. A better method for obtaining \\(\\lambda\\) is to compute a model-level metric that we can then evaluate across the models produced by the different values of \\(\\lambda\\). One such metric is the Akiake Information Criteria (AIC). We can compute the AIC for a ridge regression as\n\\[\n\\mathrm{AIC} = n \\times \\ln\\big(\\mathbf{e}^{\\intercal}\\mathbf{e}\\big) + 2(\\mathit{df})\n\\]\nwhere n is the sample size, \\(\\mathbf{e}\\) is the vector of residuals from the ridge model, and df is the degrees of freedom associated with the ridge regression model, which we compute by finding the trace of the H matrix, namely,\n\\[\ntr(\\mathbf{H}_{\\mathrm{Ridge}}) =  tr(\\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^{\\intercal})\n\\]\nFor example, to compute the AIC value associated with the ridge regression estimated using a \\(\\lambda\\) value of 0.1 we can use the following syntax.\n\n# Compute coefficients for ridge model\nb = solve(t(X) %*% X + 0.1*diag(3)) %*% t(X) %*% y\n\n# Compute residual vector\ne = y - (X %*% b)\n\n# Compute H matrix\nH = X %*% solve(t(X) %*% X + 0.1*diag(3)) %*% t(X)\n\n# Compute df\ndf = sum(diag(H))\n\n# Compute AIC\naic = 70 * log(t(e) %*% e) + 2 * df\naic\n\n         [,1]\n[1,] 285.8729\n\n\nWe want to compute the AIC value for every single one of the models associated with the \\(\\lambda\\) values from our sequence we used to produce the ridge trace plot. To do this, we will create a function that will compute the AIC from a given \\(\\lambda\\) value.\n\n# Function to compute AIC based on inputted lambda value\nridge_aic = function(lambda){\n  b = solve(t(X) %*% X + lambda*diag(3)) %*% t(X) %*% y\n  e = y - (X %*% b)\n  H = X %*% solve(t(X) %*% X + lambda*diag(3)) %*% t(X)\n  df = sum(diag(H))\n  n = length(y)\n  aic = n * log(t(e) %*% e) + 2 * df\n  return(aic)\n}\n\n# Try function\nridge_aic(lambda = 0.1)\n\n         [,1]\n[1,] 285.8729\n\n\nTo be able to evaluate which \\(\\lambda\\) value is assocuated with the lowest AIC, we need to compute the AIC for many different \\(\\lambda\\) values. To do this, we will create a data frame that has a column that includes the \\(\\lambda\\) values we want to evaluate. Then we use the rowwise() and mutate() functions to apply the ridge_aic() function to each of the lambda values. Finally, we can use filter() to find the \\(\\lambda\\) value associated with the smallest AIC value.\n\n# Create data frame with column of lambda values\n# Create a new column by usingthe ridge_aic() function for each row\nmy_models = data.frame(\n  Lambda = seq(from = 0, to = 100, by = 0.01)\n  ) %>%\n  rowwise() %>%\n   mutate(\n    AIC = ridge_aic(Lambda)\n  ) %>%\n  ungroup() #Turn off the rowwise() operation\n\n# Find lambda associated with smallest AIC\nmy_models %>% \n  filter(AIC == min(AIC))\n\n\n\n  \n\n\n\nA \\(\\lambda\\) value of 22.36 produces the smallest AIC value, so this is the \\(\\lambda\\) value we will adopt.\n\n# Re-fit ridge regression using lambda = 22.36\nridge_smallest_aic = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, \n                              lambda = 22.36)\n\n# View coefficients\ntidy(ridge_smallest_aic)\n\n\n\n  \n\n\n\nBased on using \\(\\lambda=22.36\\), the fitted ridge regression model is:\n\\[\n\\hat{\\mathrm{Achievement}}^{\\star}_i = 0.115(\\mathrm{Faculty}^{\\star}_i) + 0.173(\\mathrm{Peer}^{\\star}_i) + 0.099(\\mathrm{School}^{\\star}_i)\n\\]"
  },
  {
    "objectID": "notes/10-biased-estimation-ridge-regression.html#estimating-sampling-variance",
    "href": "notes/10-biased-estimation-ridge-regression.html#estimating-sampling-variance",
    "title": "📝 Biased Estimation: Ridge Regression",
    "section": "Estimating Sampling Variance",
    "text": "Estimating Sampling Variance\nIn theory it is possible to obtain the sampling variances for the ridge regression coefficients using matrix algebra:\n\\[\n\\sigma^2_{\\mathbf{b}} = \\sigma^2_{e}(\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^\\intercal \\mathbf{X} (\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\n\\]\nwhere \\(\\sigma^2_e\\) is the the error variance estimated from the standardized OLS model.\n\n# Fit standardized model to obtain sigma^2_e\nglance(lm(achievement ~ -1 + faculty + peer + school, data = z_data))\n\n\n\n  \n\n\n# Compute sigma^2_epsilon\nresid_var = 0.9041214 ^ 2\n\n# Compute variance-covariance matrix of ridge estimates\nW = solve(t(X) %*% X + 22.36*diag(3))\nvar_b = resid_var * W %*% t(X) %*% X %*% W\n\n# Compute SEs\nsqrt(diag(var_b))\n\n   faculty       peer     school \n0.05394529 0.05576954 0.04089317 \n\n\nComparing these SEs to the SEs from the OLS regression:\n\n\n\n\nComparison of the standard errors from the OLS ($\\lambda=0$) and ridge regression ($\\lambda=22.36$) based on the standardized data.\n \n  \n    Predictor \n    $\\lambda=0$ \n    $\\lambda=22.36$ \n  \n \n\n  \n    Faculty \n    0.667 \n    0.054 \n  \n  \n    Peer \n    0.598 \n    0.056 \n  \n  \n    School \n    0.993 \n    0.041 \n  \n\n\n\n\n\nBased on this comparison, we can see that the standard errors from the ridge regression are quite a bit smaller than those from the OLS. We can then use the estimates and SEs to compute t- and p-values, and confidence intervals. Here we only do it for the school facilities predictor (since it is of primary interest based on the RQ) but one could do it for all the predictors.\n\n# Compute t-value for school predictor\nt = 0.0998967 / 0.04089317  \nt\n\n[1] 2.44287\n\n# Compute df residual\nH = X %*% solve(t(X) %*% X + 22.36*diag(3)) %*% t(X)\ndf_model = sum(diag(H))\ndf_residual = 69 - df_model\n\n# Compute p-value\np = pt(-abs(t), df = df_residual) * 2\np\n\n[1] 0.01717516\n\n# Compute CI\n0.0998967 - qt(p = 0.975, df = df_residual) * 0.04089317  \n\n[1] 0.01829489\n\n0.0998967 + qt(p = 0.975, df = df_residual) * 0.04089317  \n\n[1] 0.1814985\n\n\nThis suggests that after controlling for peer influence and faculty credential, there is evidence of an effect of school facilities on student achievement (\\(p=.017\\)). The uncertainty in the 95% CI suggests that the true partial effect of school facilities is between 0.018 and 0.181. The empirical evidence is pointing toward a slight positive effect of school facilities."
  },
  {
    "objectID": "readings/01-welcome-to-8264.html",
    "href": "readings/01-welcome-to-8264.html",
    "title": "📖 Welcome to EPsy 8264",
    "section": "",
    "text": "In this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning.\nThe TAs and I will do everything we can to help with this, but, as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class."
  },
  {
    "objectID": "readings/01-welcome-to-8264.html#prerequisites",
    "href": "readings/01-welcome-to-8264.html#prerequisites",
    "title": "📖 Welcome to EPsy 8264",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe pre-requisites for this course are EPsy 8251 and EPsy 8252. Prerequisite knowledge include topics from a basic statistics course:\n\nFoundational topics in data analysis;\n\nDesign (e.g., random assignment and random sampling)\nDescriptive statistics and plots\nOne- and two-sample tests\n\n\nAnd, topics from EPsy 8251: Methods in Data Analysis for Educational Research I:\n\nStatistical Computation\n\nUsing R\nData wrangling/manipulation\nPlotting\n\nCorrelation;\nSimple regression analysis;\n\nModel-level and coefficient-level interpretation\nOrdinary least squares estimation\nStandardized regression\nPartitioning sums of squares\nModel-level and coefficient-level inference\nAssumption checking/residual analysis\n\nMultiple linear regression\n\nModel-level and coefficient-level interpretation and inference\nAssumption checking/residual analysis\nWorking with categorical predictors (including adjusting p-values for multiple tests)\nInteraction effects\n\n\nAnd topics from EPsy 8252: Methods in Data Analysis for Educational Research II:\n\nDealing with nonlinearity;\n\nQuadratic effects\nLog-transformations\n\nProbability distributions;\n\nProbability density\n\nMaximum likelihood estimation;\nModel selection;\n\nInformation criteria\n\nLinear mixed-effects models (cross-sectional/longitudinal)\n\nBasic ideas of mixed-effects models\nFitting models with random-intercepts and random-slopes\nAssumptions\nLikelihood ratio tests\n\nGeneralized linear models\n\nLogistic models"
  },
  {
    "objectID": "readings/01-welcome-to-8264.html#resources",
    "href": "readings/01-welcome-to-8264.html#resources",
    "title": "📖 Welcome to EPsy 8264",
    "section": "Resources",
    "text": "Resources\nFor the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. If you need a refresher on any of these topics, see:\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists [EPsy 8251 material]\nEPsy 8252 website"
  },
  {
    "objectID": "readings/02-introduction-to-matrix-algebra.html",
    "href": "readings/02-introduction-to-matrix-algebra.html",
    "title": "📖 Introduction to Matrix Algebra",
    "section": "",
    "text": "Introduction\nData Structures\nVectors\nVector Operations\n\nIn class, we will be working through some problems to cement these ideas. We will also examine and work through some problems related to matrix operations, so you could also read through:\n\nMatrices\nMatrix Addition and Subtraction\nMatrix Multiplication\nMatrix Transposition"
  },
  {
    "objectID": "readings/03-ols.html",
    "href": "readings/03-ols.html",
    "title": "📖 OLS Regression using Matrices and its Properties",
    "section": "",
    "text": "Systems of Equations\nStatistical Application: Estimating Regression Coefficients\n\nIn class, we will be working through the ideas in the following chapters:\n\nImportant Matrices in Regression\nSums of Squares in Regression\nStandard Errors and Variance Estimates\nAssumptions of the Regression Model"
  },
  {
    "objectID": "readings/04-simulating-from-the-regression-model.html",
    "href": "readings/04-simulating-from-the-regression-model.html",
    "title": "📖 Simulating from the Regression Model",
    "section": "",
    "text": "Introduction: This chapter gives a short introduction to the use of simulation in the social sciences."
  },
  {
    "objectID": "readings/05-regression-diagnostics.html",
    "href": "readings/05-regression-diagnostics.html",
    "title": "📖 Regression Diagnostics",
    "section": "",
    "text": "Fox, J. (1991). Outlying and influential data. In Regression diagnostics (pp. 21–40). Sage. doi: https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781412985604.n4\n\n\n\nAdditional Resources\n\nKim, B. (2015). Understanding diagnostic plots for linear regression analysis. University of Virginia Library.\nCook, R. D. (1998). Regression graphics: Ideas for studying regressions through graphics. Wiley."
  },
  {
    "objectID": "readings/06-variance-stabilizing-transformations.html",
    "href": "readings/06-variance-stabilizing-transformations.html",
    "title": "📖 Variance Stabilizing Transformations",
    "section": "",
    "text": "Osborne, J. W. (2009). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6). https://doi.org/10.7275/4vng-5608\n\n\n\nAdditional Resources\n\nKaufman, R. L. (2013). Heteroskedasticity in regression: Detection and correction. Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4"
  },
  {
    "objectID": "readings/07-wls-and-sandwich-estimation.html",
    "href": "readings/07-wls-and-sandwich-estimation.html",
    "title": "📖 Weighted Least Squares (WLS) and Sandwich Estimation",
    "section": "",
    "text": "Read Kaufman, R. L. (2013). Heteroskedasticity-consistent (robust) standard errors. In Heteroskedasticity in regression: Detection and correction (pp. 43–50). Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\n\n\nAdditional Resources\n\nShin, H.-C. (1998). Weighted least squares estimation with sampling weights. Journal of Econometrics, 8(2), 251–271.\nSolon, G., Haider, S. J., & Woolridge, J. (2013). What are we weighting for? (Working Paper No. 18859; NBER Working Paper Series). National Bureau of Economic Research."
  },
  {
    "objectID": "readings/08-diagnosing-collinearity.html",
    "href": "readings/08-diagnosing-collinearity.html",
    "title": "📖 Diagnosing Collinearity",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Eigenvalues and Eigenvectors. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nCook, D. (2019). How to use a tour to check if your model suffers from multicollinearity. Personal blog."
  },
  {
    "objectID": "readings/09-pca-via-spectral-decomposition.html",
    "href": "readings/09-pca-via-spectral-decomposition.html",
    "title": "📖 Principal Components Analysis via Spectral Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Basis vectors and matrices. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and eigenvectors. In Matrix algebra for educational scientists.\n\nRodriguez, M., & Zieffler, A. (2021). Spectral decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nWilke, C. O. (2020). PCA tidyverse style. Personal blog."
  },
  {
    "objectID": "readings/10-pca-via-svd.html",
    "href": "readings/10-pca-via-svd.html",
    "title": "📖 Principal Components Analysis via Singular Value Decomposition",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Singular value decomposition. In Matrix algebra for educational scientists.\n\n\n\nAdditional Resources\n\nGundersen, G. (2018). Singular value decomposition as simply as possible.\nWang, Z. (2019). PCA and SVD explained with numpy.\nWikipdeia. (2020). Singular value decomposition."
  },
  {
    "objectID": "readings/11-biased-estimation-ridge-regression.html",
    "href": "readings/11-biased-estimation-ridge-regression.html",
    "title": "📖 Biased Estimation: Ridge Regression",
    "section": "",
    "text": "Fortmann-Roe, S. (2012). Understanding the bias-variance tradeoff.\n\n\n\nAdditional Resources\n\nCross-Validated. (2014). Why is ridge regression called “ridge”, why is it needed, and what happens when \\(\\lambda\\) goes to infinity?.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.\nStatistical Learning MOOC taught by Hastie and Tibshirani"
  },
  {
    "objectID": "readings/12-model-selection.html",
    "href": "readings/12-model-selection.html",
    "title": "📖 Model Selection",
    "section": "",
    "text": "Heinze, G., Wallisch, C., & Dunkler, D. (2018). Variable selection — A review and recommendations for the practicing statistician. Biometrical Journal. Biometrische Zeitschrift, 60(3), 431–449. https://doi.org/10.1002/bimj.201700067\n\n\n\nAdditional Resources\n\nolsrr Vignette\nWilliams, B., Hansen, G., Baraban, A., & Santoni, A. (2015). A practical approach to variable selection—A comparison of various techniques. Casualty Actuarial Society E-Forum, Summer, 1–20."
  },
  {
    "objectID": "readings/13-cross-validation.html",
    "href": "readings/13-cross-validation.html",
    "title": "📖 Cross-Validation",
    "section": "",
    "text": "Feick, L. (2019). Evaluating model performance by building cross-validation from scratch. STATWORX Blog.\n\n\n\nAdditional Resources\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). Cross-validation. In An introduction to statistical learning: with applications in R (pp. 176–186). New York: Springer."
  },
  {
    "objectID": "readings/14-regression-from-summary-measures.html",
    "href": "readings/14-regression-from-summary-measures.html",
    "title": "📖 Regression from Summary Measures",
    "section": "",
    "text": "Rodriguez, M., & Zieffler, A. (2021). Statistical application: SSCP, variance–covariance, and correlation matrices.. Matrix Algebra for Educational Scientists.\n\nIn particular pay attention to the relationships between these three matrices."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Calendar",
    "section": "",
    "text": "Unit 04: COLLINEARITY\n\n\n\n\n\nDate\n\n\nReading\n\n\nTopic\n\n\nNotes\n\n\n\n\n\n\n \n\n\nSept. 06\n\n\n\n\n\nWelcome to EPsy 8264\n\n\n \n\n\n\n\n\nUnit 01: Mathematical and Computational Foundations\n\n\n\n\n \n\n\nSept. 08\n\n\n\n\n\nIntroduction to Matrix Algebra\n\n\n\n\n\n\n\n \n\n\nSept. 13\n\n\n\n\n \n\n\nSept. 15\n\n\n\n\n\nOLS Regression using Matrices and its Properties\n\n\n\n\n\n\n\n \n\n\nSept. 20\n\n\n\n\n\nUnit 02: Simulation\n\n\n\n\n \n\n\nSept. 22\n\n\n\n\n\nSimulating from the Regression Model\n\n\n\n\n\n\n\n \n\n\nSept. 27\n\n\n\n\n \n\n\nSept. 29\n\n\n\n\n\nUnit 03: Regression Diagnostics\n\n\n\n\n \n\n\nOct. 04\n\n\n\n\n\nRegression Diagnostics\n\n\n\n\n\n\n\n \n\n\nOct. 06\n\n\n\n\n\nUnit 04: Tools for Dealing with Heteroskedasticity\n\n\n\n\n \n\n\nOct. 11\n\n\n\n\n\nVariance Stabilizing Transformations\n\n\n\n\n\n\n\n \n\n\nOct. 13\n\n\n\n\n\nWeighted Least Squares (WLS) and Sandwich Estimation\n\n\n\n\n\n\n\n\nUnit 05: Diagnosing Collinearity and Tools for Dealing with It\n\n\n\n\n \n\n\nOct. 18\n\n\n\n\n\nDiagnosing Collinearity\n\n\n\n\n\n\n\n \n\n\nOct. 20\n\n\n\n\n\nPrincipal Components Analysis via Spectral Decomposition\n\n\n\n\n\n\n\n \n\n\nOct. 25\n\n\n\n\n\nPrincipal Components Analysis via Singular Value Decomposition\n\n\n\n\n\n\n\n \n\n\nOct. 27\n\n\n\n\n\nPrincipal Components on Different Input Matrices\n\n\n\n\n\n\n\n \n\n\nNov. 01\n\n\n\n\n\nBiased Estimation: Ridge Regression\n\n\n\n\n\n\n\n \n\n\nNov. 02\n\n\n\n\n \n\n\nNov. 08\n\n\n\n\nNO CLASS—Mental Health Break/Andy at a Conference"
  },
  {
    "objectID": "worksheets/02-introduction-to-matrix-algebra.html",
    "href": "worksheets/02-introduction-to-matrix-algebra.html",
    "title": "💪 Introduction to Matrix Algebra",
    "section": "",
    "text": "Problems\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & -2\\\\5 & 1\\end{bmatrix} \\quad \\mathbf{B} = \\begin{bmatrix}3 & -1\\\\-1 & 2\\end{bmatrix}\\quad \\mathbf{C} = \\begin{bmatrix}1 & 2 & 3\\\\0 & 1 & 2\\end{bmatrix}\n\\]\nMake sure everyone in your group can solve each of these problem by hand and using R.\n\nWhat are the dimensions of A? C?\nIs C a square matrix? Explain.\nFind the trace of A.\nFind the determinant of A.\nAdd A and B\nFind the transpose of C.\nBy referring to the dimensions, can you compute AC? How about CA?\nCompute AC.\nCompute BI\nCreate a \\(3\\times3\\) diagonal matrix whose trace is 10.\nHow do you know that B has an inverse? Explain.\nCompute \\(\\mathbf{B}^{-1}\\)\nCreate a \\(3\\times3\\) matrix that has rank 2. Verify this using R.\nCreate a \\(3\\times3\\) matrix that is symmetric and is not I.\nSolve the system of linear equations using algebra (e.g., substitution, elimination) and then solve them using matrix methods (with R). To do this you will need to read the Systems of Equations chapter in Matrix Algebra for Educational Scientists.\n\n\\[\n\\begin{split}\nx + y + z &= 2 \\\\\n6x - 4y + 5z &= 31 \\\\\n5x + 2y + 2z &= 13\n\\end{split}\n\\]"
  },
  {
    "objectID": "worksheets/13-model-selection.html",
    "href": "worksheets/13-model-selection.html",
    "title": "💪 Model Selection (In-Class Activity)",
    "section": "",
    "text": "[CSV]\n[Data Codebook]\n\n\nYour goal is to use the data (and everything you’ve learned so far in your coursework) to create a model to predict variation in life expectancy. Keep track of the process you use to create this model, including:\n\nWhich predictors should be included in the model?\n\nWhat is the criteria/evidence you are using to make these decisions?\n\nWhen in the process do you identify problematic observations?\n\nDo you remove those problematic observations or not?\nWhat criteria/evidence are you using to make these decisions?\n\nWhen in the process do you examine the model for collinearity?\n\nWhat is the criteria/evidence you are using to make this decision?\nWhat (if anything) will you do to fix this?\n\nWhen in the process do you examine the tenability of assumptions?\n\nAlso pay attention to when in the process you are making decisions based on sample evidence (graphs/statistics) versus when those decisions are being made using statistical inference (hypothesis tests, confidence intervals). Your group will be asked to report back to the class on the process, criteria, and evidence you used."
  }
]